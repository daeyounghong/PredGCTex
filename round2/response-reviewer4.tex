\documentclass[11pt]{article}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{xr}
\usepackage{cleveref}

\externaldocument{gc-transfer}

\geometry{margin=1in}

\begin{document}

\begin{center}
{\Large \textbf{Responses to Reviewer 4}} \\[6pt]
%Manuscript ID: \textbf{diagnostics-3950051} \\
%Title: \textit{Cross-Cancer Transfer Learning for Gastric Cancer Risk Prediction from Electronic Health Records}
\end{center}

\vspace{1em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Summary}
We sincerely thank the reviewers for their careful reading of our manuscript and for providing insightful comments that helped us improve the clarity and rigor of the work. Below we provide detailed, point-by-point responses to all comments. All revisions are highlighted in the revised manuscript in \textcolor{red}{red}, and the locations of the changes are indicated in each response.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{}

\subsection*{Comment 1}
First, the validation is limited to a single cohort derived from the MIMIC-IV database. This limitation is understandable but makes it difficult to assess the model’s generalizability to other settings. I suggest that the authors elaborate further on how a future multicenter or international validation could affect the model’s transferability and potential integration into hospital EHR systems.

\subsection*{Response 1}
We thank the reviewer for this important comment regarding generalizability
beyond the single MIMIC-IV cohort. As suggested, we have expanded the
paragraph in the Discussion section that describes the study’s limitations
(pp.~15--16, lines~585--594) to elaborate on how future multicenter or international
validation could inform the model’s transferability and EHR integration.
In particular, we now explicitly discuss that multicenter and international
studies would allow us to measure performance and calibration drift across
hospitals with different patient populations and workflows, and to compare
strategies such as site-specific recalibration versus fine-tuning of the
shared encoder. We also clarify that the results of such studies would guide
whether a single global model with site-specific decision thresholds is
sufficient, or whether locally adapted models are
needed for safe integration into hospital EHR systems.



\subsection*{Comment 2}
Second, while the improvement over the baseline MLP model is consistent, the absolute gain in predictive metrics (AUROC +0.024) is quantitatively modest. It would be valuable to clarify, perhaps within the Discussion, to what extent such a difference could translate into tangible clinical benefit — for instance, in prioritizing patients for endoscopy or stratifying high-risk populations. A short remark on possible decision-curve or clinical-impact analyses would strengthen the translational message of the study.


\subsection*{Response 2}
We thank the reviewer for this insightful comment. 
In the revised Discussion, we explicitly acknowledge that the improvement of the Transfer model over the scratch MLP on the full-label GC task is modest in absolute terms (AUROC 0.854 vs 0.830) and emphasize that our main contribution lies in the model’s stability and behavior under label scarcity (p.~13, lines~463--471).
We also clarify how the observed differences in AUROC, F1, and specificity could affect clinical decision-making when GC risk scores are used to prioritize patients for upper endoscopy or to stratify high-risk populations. 
Furthermore, we have added a short remark highlighting that we did not perform a formal decision-curve or clinical-impact analysis in the present study, and that future work should quantify the net clinical benefit of using the proposed model to guide endoscopy referrals (pp.~13--14, lines~478--492).


\subsection*{Comment 3}
I found the Limitations section well balanced, but it would benefit from a slightly deeper reflection on the potential selection bias introduced by the “lab-complete” criterion, which excludes patients with missing laboratory data and may alter the underlying risk distribution. A brief comment on the likely direction of this bias (e.g., a possible overestimation of model performance in cohorts with denser testing) would help readers interpret the reported results.

\subsection*{Response 3}
We thank the reviewer for this insightful comment. We fully agree that the “lab-complete” criterion can introduce selection bias and that clarifying the likely direction of this bias is important for interpreting the reported performance metrics. In the revised Discussion, we have expanded the paragraph on the lab-complete cohort to highlight that, in our setting, it is likely to induce an optimistic bias in the estimated performance (p.~16, lines~617--627).



\subsection*{Comment 4}
Finally, I recommend integrating the following reference into the Introduction to broaden the contextual background: \textbf{Palomba et al., \textit{Artificial intelligence in screening and diagnosis of surgical diseases: A narrative review, AIMS Public Health}, 2024}. This article provides an updated overview of artificial intelligence in surgical disease screening and diagnosis, including gastrointestinal disorders, and would help situate the current work within the broader landscape of AI-driven predictive medicine and surgical oncology.

\subsection*{Response 4}
We thank the reviewer for this helpful suggestion. 
In the revised Introduction, we now cite and discuss the narrative review by Palomba et al.\ (pp.~1--2, lines~32--38), to explicitly situate our EHR-based gastric cancer risk prediction model within the broader context of AI-driven predictive medicine and surgical oncology.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliographystyle{plainnat}
%\bibliography{references} % Replace ‘references.bib’ with your bib file name.

\end{document}
