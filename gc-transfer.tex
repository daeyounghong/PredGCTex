%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[diagnostics,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aieduc, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, amh, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, glacies, grasses, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, iic, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdad, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joitmc, joma, jop, jor, journalmedia, jox, jpbi, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidney, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, benchmark, book, bookreview, briefcommunication, briefreport, casereport, changes, clinicopathologicalchallenge, comment, commentary, communication, conceptpaper, conferenceproceedings, correction, conferencereport, creative, datadescriptor, discussion, entry, expressionofconcern, extendedabstract, editorial, essay, erratum, fieldguide, hypothesis, interestingimages, letter, meetingreport, monograph, newbookreceived, obituary, opinion, proceedingpaper, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, supfile, systematicreview, technicalnote, viewpoint, guidelines, registeredreport, tutorial,  giantsinurology, urologyaroundtheworld
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}
\usepackage{kotex}

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Early Prediction of Gastric Cancer Using Transfer Learning from Non-Gastric Cancers}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Title}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0003-3002-1972} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-0000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Daeyoung Hong $^{1,}$*\orcidA{}, Jiung Kim $^{1}$ and Jiyong Jung $^{1}$}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Daeyoung Hong, Jiung Kim and Jiyong Jung}

% MDPI internal command: Authors, for citation in the left column, only choose below one of them according to the journal style
% If this is a Chicago style journal 
% (arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci): 
% Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% If this is a APA style journal 
% (admsci, behavsci, businesses, econometrics, economies, education, ejihpe, games, humans, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth): 
% Lastname, F., Lastname, F., \& Lastname, F.

% If this is a ACS style journal (Except for the above Chicago and APA journals, all others are in the ACS format): 
% Lastname, F.; Lastname, F.; Lastname, F.
\isAPAStyle{%
       \AuthorCitation{Lastname, F., Lastname, F., \& Lastname, F.}
         }{%
        \isChicagoStyle{%
        \AuthorCitation{Lastname, Firstname, Firstname Lastname, and Firstname Lastname.}
        }{
        \AuthorCitation{Lastname, F.; Lastname, F.; Lastname, F.}
        }
}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address[1]{%
School of Software Convergence, Myongji University, Seoul 03674, Republic of Korea; jiung8758@gmail.com (J.K.); jiyongj954@gmail.com (J.J.)}
%\\$^{2}$ \quad Affiliation 2; e-mail@e-mail.com}

% Contact information of the corresponding author
\corres{Correspondence: dyhong@mju.ac.kr}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation.}  
% Current address should not be the same as any items in the Affiliation section.

%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes.

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{A single paragraph of about 200 words maximum. For research articles, abstracts should give a pertinent overview of the work. We strongly encourage authors to use the following style of structured abstracts, but without headings: (1) Background: place the question addressed in a broad context and highlight the purpose of the study; (2) Methods: describe briefly the main methods or treatments applied; (3) Results: summarize the article's main findings; (4) Conclusions: indicate the main conclusions or interpretations. The abstract should be an objective representation of the article, it must not contain results which are not presented and substantiated in the main text and should not exaggerate the main conclusions.}

% Keywords
\keyword{keyword 1; keyword 2; keyword 3 (List three to ten pertinent keywords specific to the article; yet reasonably common within the subject discipline.)} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal BioTech, Fishes, Neuroimaging and Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Advances in Respiratory Medicine, Future, Sensors and Smart Cities
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%
%
%\noindent This is an obligatory section in ``Advances in Respiratory Medicine'', ``Future'', ``Sensors'' and ``Smart Cities”, whose goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{document}
\newcommand{\GC}{\mathrm{GC}}
\newcommand{\CC}{\mathrm{CC}}
\newcommand{\EC}{\mathrm{EC}}
\newcommand{\LC}{\mathrm{LC}}
\newcommand{\PC}{\mathrm{PC}}
\newcommand{\encP}{\phi}  % encoder parameters
\newcommand{\enc}[1]{f_{#1}}
\newcommand{\headP}[1]{\theta_{#1}}
\newcommand{\head}[1]{h_{\headP{#1}}}
\newcommand{\pretrainP}{\encP^\star}
\newcommand{\preDecayC}{\lambda_{\mathrm{pre}}}  % pretraining decay coefficient
\newcommand{\ftDecayC}{\lambda_{\mathrm{ft}}}  % FT decay coefficient
\section{Introduction}
Gastric cancer (GC) continues to pose a major challenge to global health, 
ranking as the fifth most frequently diagnosed malignancy and the fourth leading cause of cancer-related death worldwide~\citep{park2024SHapley}. 
Prognosis remains strongly dependent on the stage at diagnosis, 
with 5-year survival rates exceeding 70\% for localized disease but dropping below 10\% for metastatic GC~\citep{NCI2023stomach}.

Harnessing routinely collected electronic health record (EHR) data for earlier GC identification could improve outcomes by enabling timely endoscopy and treatment.
Structured EHR signals—such as demographics, diagnoses, and laboratory data—represent an attractive source for data-driven cancer screening~\citep{read2023cancers,huang2022jco}.
However, EHR-based model development can face persistent challenges, including incomplete or missing records, irregular sampling intervals, and the overall complexity of clinical documentation.
Moreover, ethical and regulatory requirements—such as institutional approvals or patient consent—can further limit the volume of analyzable data, 
leading to relatively small, heterogeneous datasets for model training.
% TODO: citation

For prediction of gastric cancer, machine learning models such as logistic regression, XGBoost, using routine and structured variables (e.g., demographics, diagnoses or common labs), have been investigated ~\citep{park2024SHapley,huang2022jco,Kim2024EHRGC}.
Despite these advances, most prior efforts optimize \emph{within-cancer} prediction and do not test whether \emph{cross-cancer} regularities in structured EHR signals (e.g., anemia/inflammation and metabolic axes) can be distilled and transferred to improve GC modeling under data scarcity.

Across gastrointestinal (GI) and hepatopancreatobiliary (HPB) malignancies, multiple EHR-visible laboratory and clinical signatures overlap with those leveraged for GC detection. 
Luminal GI tract tumors often cause gradual occult bleeding that culminates in iron-deficiency anemia (IDA), producing characteristic shifts in longitudinal complete blood count (CBC) measures~\citep{read2023cancers,Aksoy2019CBC,Krieg2024IDA,Kim2014GIBleed}. 
In liver disease and primary liver cancer, anemia and systemic inflammation/nutritional markers (e.g., albumin, C-reactive protein, neutrophil–lymphocyte and platelet–lymphocyte ratios) are prominent and are also established biomarkers of GC prognosis~\citep{Stein2016Anemia,Gkamprela2017CLDIDA,Crumley2010Albumin,Kim2020Inflam}. 
Pancreatic cancer frequently perturbs glucose metabolism; dysglycemia and diabetes are associated with GC risk, suggesting that encoders exposed to glucose/HbA1c dynamics may learn representations transferable to GC without implying causation~\citep{Shimoyama2013DMGC,Yoon2013DMGC,Guo2022DMGC}. 
These cross-cancer regularities motivate leveraging non-GC cohorts to improve sample efficiency for GC modeling.

Deep learning (DL) provides a principled way to learn non-linear interactions from tabular EHR variables and to distill noisy laboratories into task-relevant representations. 
When labels for GC are limited, transfer learning (TL) and multi-task learning (MTL) mitigate data scarcity by first inducing \emph{shared, cancer-agnostic representations} on related cancers and then adapting them to GC~\citep{Caruana1997_MTL,Pan2010_TLsurvey,Yosinski2014_transferable}. 
In this paradigm, DL functions primarily as a \emph{representation learner} that captures cross-cancer laboratory signatures expected to recur in GC, thereby improving sample efficiency and stability at adaptation time.

Accordingly, we aimed to investigate a cross-cancer transfer-learning paradigm with deep learning based on EHR data: we pre-train a shared multilayer-perceptron (MLP) backbone on non-gastric GI/HPB cancers using routinely collected EHR variables (demographics, diagnoses/ICD codes, and laboratory values) and then adapt it to GC via fine-tuning. 
We evaluate this hypothesis retrospectively in the MIMIC-IV v3.1 EHR database \citep{Johnson2024MIMICIV}, restricting inputs to structured EHR data available without using diagnostic endoscopy or pathology (i.e., no clinical notes), to minimize surveillance bias and label leakage.

%Our contributions are threefold:
%(i) a multi-head transfer MLP with structural-parameter (L2-SP) regularization \citep{li2018l2sp} and controllable fine-tuning heads (including logit averaging) to stabilize adaptation; and
%(ii) a rigorous, seed-repeated selection protocol that fixes model choice by validation AUROC and then reports both validation and held-out test metrics with sensitivity/specificity/F1 at a validation-optimized threshold \citep{davis2006relationship,saito2015precision}.

%To test this hypothesis, we developed a reproducible pipeline on the MIMIC-IV v3.1 database to construct cohorts, engineer biologically meaningful features, and train several baseline models along with a transfer-learning multilayer perceptron (Transfer-MLP). The Transfer-MLP is pretrained on non-GC GI cancers (colorectal, liver, esophageal, pancreatic) and then fine-tuned on GC labels using a controlled head-selection strategy and structural parameter (SP) regularization. Our contributions are: (i) a transparent, fully code-driven cohort construction and feature engineering workflow that avoids features merely encoding physicians' suspicion; (ii) a multi-head transfer architecture that supports different fine-tuning strategies; and (iii) an empirical demonstration that pretraining on related cancers improves early GC prediction compared with GC-only training.


\section{Materials and Methods}

\subsection{Data Source and Ethics}
We used the publicly available de-identified MIMIC-IV clinical database (v3.1) \citep{Johnson2024MIMICIV}.
Access to the database was granted to our research team, and we adhered to all usage agreements, including the prohibition of data disclosure and misuse.

\subsection{Cohort Construction and Features}
In this study, we constructed cohorts not only for gastric cancer (GC) but also for multiple non-GC cancers (colorectal, esophageal, liver, and pancreatic).
This design emphasizes that our approach leverages diverse cancer types for pre-training before fine-tuning on GC.
Throughout, we use the notations to denote cancer types: \textbf{GC} = gastric cancer (target task), \textbf{CC} = colorectal cancer, \textbf{EC} = esophageal cancer, \textbf{LC} = liver cancer, and \textbf{PC} = pancreatic cancer.

For each cancer, we identified cases by the earliest qualifying ICD-9/10 diagnosis and sampled controls at a fixed $1{:}3$ case:control ratio, excluding subjects with the target cancer.
The GC index time $t_G$ is the first GC diagnosis; for non-GC cancers $c\in\{\mathrm{CC},\mathrm{EC},\mathrm{LC},\mathrm{PC}\}$ we analogously define $t_c$.
Features include demographics (age, sex), binary ICD-derived comorbidity indicators (e.g., diabetes, GERD, ulcers), and the most recent values of routine laboratory tests (CMP/CBC) in a $\pm 30$-day window around the index time (or all available values for controls).
Identical preprocessing is applied to cases and controls; numerical features are standardized by removing the mean and scaling to unit variance using statistics on the training split only.

To address missing values, we applied median imputation based on the control population. 
For each feature, the median value observed among the controls was calculated and substituted for missing entries across both cases and controls. 
This strategy minimizes information leakage from the case group and provides a robust central tendency measure for imputation.
%TODO: finally used feature list



%\subsection{Feature Engineering}
%We engineered features that are interpretable and clinically plausible. Demographics included age and sex. Binary ICD-derived comorbidity indicators were computed for clinically relevant conditions (e.g., diabetes, obesity, GERD, dyspepsia, gastric ulcer, alcohol/tobacco use, anemia, malnutrition, atrial fibrillation, renal failure, lipid disorders). For laboratory features, we included the most recent value per analyte within a symmetric look-back window around the index date (or all available values for controls), covering complete blood count indices and common chemistries (e.g., hemoglobin, hematocrit, MCV/MCH/MCHC, WBC and differentials, platelets, RDW; sodium, potassium, chloride, bicarbonate, urea nitrogen, creatinine, glucose, anion gap). Missing values were imputed and missingness flags optionally dropped after imputation. Non-predictive index-related variables were excluded from modeling.


\subsection{Models}
Our primary objective is to \textbf{quantify how much transfer learning from non-gastric cancers improves early prediction of gastric cancer (GC)}. 
We first introduce the \emph{transfer learning framework}, which pre-trains models on multiple non-GC cancers and subsequently fine-tunes them on GC.
We then describe the \emph{non-transfer baselines} trained solely on GC data for direct comparison.
To isolate the effect of transfer, we enforce identical feature sets, preprocessing steps, and stratified data splits across all methods.

\subsubsection{Transfer MLP}
Let $\mathcal{C}$ denote the set of \emph{non-gastric} cancers used for pretraining; in our main configuration, $\mathcal{C}=\{\CC,\EC,\LC,\PC\}$ (i.e., colorectal, esophageal, liver, and pancreatic cancers, respectively).
For each $c\in\mathcal{C}$, let $\mathcal{D}_c=\{(x_i^{(c)},y_i^{(c)})\}_{i=1}^{N_c}$ be the corresponding case--control dataset (identical feature space across $c$), constructed with a case--control ratio of $1{:}3$ (one case per three controls).
Here $y\in\{0,1\}$ indicates the disease status for cancer type $c$ ($y{=}1$ for \emph{case} with a confirmed diagnosis of type $c$, and $y{=}0$ for a \emph{control} without that diagnosis).

Our deep learning model consists of a shared backbone encoder $f_\phi:\mathbb{R}^{d_{\mathrm{in}}}\!\to\!\mathbb{R}^{d_h}$ with model parameters $\phi$ and cancer-specific linear heads $\{h_{\theta_c}\}_{c\in\mathcal{C}}$ acting on the latent representation $u=f_\phi(x)$ for input features $x$.
Each head with model parameters $\theta_c=(w_c,b_c)\in\mathbb{R}^{d_h}\times\mathbb{R}$ is an affine map parameterized as
\[
h_{\theta_c}(u)=\langle w_c,u\rangle + b_c,
\]
and produces a logit $z_c(x)=h_{\theta_c}(f_\phi(x))$.
The backbone encoder $f_\phi$ is a feed-forward network with linear blocks, batch normalization \citep{Ioffe2015_BN}, ReLU, and dropout~\citep{Srivastava2014_Dropout}.

We first define the class-weighted binary cross-entropy (BCE) used throughout.
Let $\sigma(t)=\frac{1}{1+\exp(-t)}$ denote the logistic sigmoid.
Given a logit $z\in\mathbb{R}$ and a binary label $y\in\{0,1\}$, the per-sample loss is
\begin{equation}
  \label{eq:bce}
  \ell_{\mathrm{BCE}}(z,y;\alpha)
  = -\,\alpha\,y\log \sigma(z)\;-\;(1-y)\log\!\big(1-\sigma(z)\big),
  \end{equation}
where $\alpha>0$ is the positive-class weight; in our experiments we set $\alpha=3$ to match the $1{:}3$ case--control sampling design.
%TODO: cite: class-weighted BCE?

During pretraining, we jointly optimize the backbone $f_\phi$ and all cancer-specific heads $\Theta_h\!=\!\{\theta_c\}_{c\in\mathcal{C}}$ by minimizing the mean of the weighted BCE over cancer types:
\begin{equation}
  \frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}}
  \ \mathbb{E}_{(x,y)\sim\mathcal{D}_c}\!\big[\,\ell_{\mathrm{BCE}}(h_{\theta_c}(f_\phi(x)),y;\alpha)\,\big],
\end{equation}
with positive-class weight $\alpha{=}3$. 
To reduce overfitting, we applied standard $\ell_2$-regularization
(weight decay) with coefficient $\preDecayC$.
Thus, the pretraining objective is
\begin{equation}
  \label{eq:pre_obj}
%  \min_{\phi,\Theta_h}\ 
  \mathcal{L}_{\mathrm{pre}}(\phi,\Theta_h)
  = \frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}}
  \mathbb{E}_{(x,y)\sim\mathcal{D}_c}\!\big[\,\ell_{\mathrm{BCE}}(h_{\theta_c}(f_\phi(x)),y;\alpha)\,\big]
  + \preDecayC \big( \|\phi\|_2^2 + \sum_{c\in\mathcal{C}} \|\theta_c\|_2^2 \big).
\end{equation}

In practice, model training does not update the parameters using the entire dataset at once, but rather processes smaller subsets of data called \emph{mini-batches}. 
A mini-batch is simply a small group of samples (e.g., a few dozen patients) drawn from the full dataset, and the model parameters are updated based on the gradient of the loss function computed by using only that group. 
This strategy reduces memory requirements and stabilizes optimization compared to using a single sample (stochastic updates) or the full dataset (full-batch updates).
During pretraining, at each optimization step we draw one mini-batch $B_c$ from \emph{each} cancer type $c\in\mathcal{C}$ and compute their respective losses. 
The pretraining loss for a step is then obtained by averaging over all cancer types:
\begin{equation}
  \label{eq:pre_mix}
  \hat{\mathcal{L}}_{\mathrm{pre}}(\phi,\Theta_h)
  =\frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}}\ \frac{1}{|B_c|}\!
  \sum_{(x,y)\in B_c}\ell_{\mathrm{BCE}}(h_{\theta_c}(f_\phi(x)),y;\alpha) 
  + \preDecayC \Big( \|\phi\|_2^2 + \sum_{c\in\mathcal{C}} \|\theta_c\|_2^2 \Big).
\end{equation}
In this way, each cancer-specific head $h_{\theta_c}$ is trained using its own mini-batch, while the backbone encoder $f_\phi$ receives gradients aggregated across cancers, encouraging it to learn shared representations.

\textit{Expected effect.} Type-balanced, multi-task pretraining (\cref{eq:pre_obj,eq:pre_mix}) is expected to (i) induce cancer-agnostic risk representations in $f_\phi$, (ii) improve sample efficiency and stabilize optimization when fine-tuning on gastric cancer with limited labels, and (iii) mitigate label-imbalance effects through the weighted loss in \cref{eq:bce}.
These mechanisms are consistent with established benefits of multi-task learning and transfer learning~\citep{Caruana1997_MTL,Pan2010_TLsurvey,Yosinski2014_transferable,He2009_Imbalanced}.

For training the prediction model for GC, we introduce new GC linear head $\head{\GC}:\mathbb{R}^{d_{h}}\!\to\!\mathbb{R}$ whose model parameters $\headP{\GC}$ are randomly initialized.
For input features $x$, by using the pretrained backbone encoder $\enc{\encP}$, the probability of GC is predicted by applying the sigmoid function $\sigma$ to the logit $\head{\GC}(\enc{\encP}(x))$ (i.e., $\sigma(\head{\GC}(\enc{\encP}(x)))$).
Let $\pretrainP$ denote the pretrained model parameters of backbone encoder.
For fine-tuning, we first initialize the model parameters of backbone $\encP$ to $\pretrainP$, and then jointly optimize the backbone $\enc{\encP}$ and the GC head $\head{\GC}$ by minimizing the weighted BCE for the GC dataset:
\begin{equation}
\label{eq:ft_obj}
\mathcal{L}_{\mathrm{ft}}(\phi,\headP{\GC}) = \mathbb{E}_{(x,y)\sim \mathcal{D}_{\GC}}\!\big[\ell_{\mathrm{BCE}}(\head{\GC}(\enc{\pretrainP}(x)),y;\alpha)\big]\;+\;\ftDecayC \big( \|\phi\|_2^2 + \|\headP{\GC}\|_2^2 \big),
\end{equation}
where we use positive-class weight $\alpha{=}3$ and $\ftDecayC$ is the $\ell_2$-regularization coefficient for fine-tuning.

%% TODO: cite Adam
%For pretraining and fine-tuning phases, we optimize model parameters using the Adam optimizer with separate learning rates and $\ell_2$-regularization coefficients $\preDecayC$ and $\ftDecayC$.
%The mini-batch size is set to $\lfloor 0.2 \cdot N_c \rfloor$ capped to $[8,64]$ where $N_c$ is the data size for cancer type $c \in \{\CC,\EC,\LC,\PC,\GC\}$.
%All tasks share the positive-class weight $\alpha = 3$ in $\ell_{\mathrm{BCE}}$.

%TODO: move the below in the section of experimental results?
%Decision thresholds are chosen on the validation set by maximizing $F_1$:
%\begin{equation}
%t^\star=\arg\max_{t\in[0,1]}\ \frac{2\,\mathrm{Precision}(t)\cdot\mathrm{Recall}(t)}{\mathrm{Precision}(t)+\mathrm{Recall}(t)}.
%\end{equation}

\subsubsection{Baselines (Non-transfer)}
We benchmark three widely used non-transfer families that reflect complementary inductive biases for structured EHR data \citep{EHRBreastRecurrence2023,WANG2023Prostate,park2024SHapley}. To avoid conflating modeling choices with pretraining, all baselines use the identical feature set, preprocessing, and stratified data splits as the Transfer MLP.

\subparagraph{Logistic Regression (LR)}
A linear probabilistic classifier that models the log-odds of GC as an affine function of the inputs, offering a strong, interpretable tabular baseline. LR captures additive effects and provides well-calibrated risk estimates under mild assumptions, making it a common comparator in clinical prediction studies.

\subparagraph{eXtreme Gradient Boosting (XGB)}
An ensemble of decision trees trained by gradient boosting to approximate non-linear decision boundaries and higher-order feature interactions on tabular data. This class of models is competitive on EHR tasks due to its flexibility with mixed-scale features and robustness to outliers and sparse signals.

\subparagraph{MLP trained from scratch (MLP)}
To isolate the effect of transfer learning, we also evaluate a multilayer perceptron that is \emph{architecturally identical} to the Transfer MLP’s backbone and GC head (linear blocks with batch normalization, ReLU, and dropout), and uses the \emph{same} training objective, mini-batch formulation, and regularization as in fine-tuning. The only difference is that the pretraining stage on non-GC cancer datasets is \emph{omitted}: all parameters are randomly initialized and optimized solely on the GC dataset. Conceptually, this baseline serves as an \emph{ablation} of the Transfer MLP in which the pretraining component is removed; thus, any performance gap directly quantifies the benefit of transfer.


\subsection{Statistical Analysis}
%In imbalanced settings, AP is often more informative than AUROC~\citep{Saito2015_PR}.
For each cancer type, we first divided the subjects into training and test sets, using an 85\%--15\% split. 
We further split the training set so that 20\% of it was used as a validation set. 
In all splits, stratified sampling was performed to ensure that the proportion of cases and controls remained consistent across the training, validation, and test sets. 
For a given hyperparameter setting, we trained model parameters using the training set, and computed the model performances for the validation and test sets. 
The evaluation metrics included the area under the receiver operating characteristic curve (AUROC), average precision (AP), sensitivity, specificity, and F1-score. 
This entire procedure was repeated 10 times with different data splits. 
To optimize hyperparameters, we computed the mean AUROC on the validation set across these repetitions and selected hyperparameters based on this mean AUROC. 
The final model performance was then evaluated on the independent test split, and we report the mean performance across the 10 repetitions as the final test result. 

Metrics such as AUROC and average precision (AP) were computed directly from the continuous prediction scores without applying any threshold. 
For threshold-dependent measures, operating points were determined using the validation split and then applied to the independent test split. 
Specifically, the threshold for the F1-score was chosen as the point that maximized the validation F1, and this same threshold was used to compute the F1-score on the test set. 
For sensitivity and specificity, following the approach in \citep{read2023cancers}, we selected the threshold that provided the best balance between the two metrics—that is, the point on the validation receiver operating characteristic (ROC) curve that was closest to the ideal classification point representing 100\% sensitivity and 0\% false-positive rate. 
All reported sensitivity and specificity values on the test set were based on this validation-derived threshold.


% TODO: decision threshold

%TODO: explaining metrics

\section{Results}
\subsection{Cohort Overview}
\begin{table}[htbp]
\caption{Cohort summary (GC target; CC/EC/LC/PC sources). Male (\%) and age median [IQR].\label{tab:cohort}}
\centering
\begin{tabularx}{\textwidth}{CCCCP{4cm}}
\toprule
Cohort & Cases & Controls & Male (\%) & Age (Median [IQR]) \\
\midrule
GC & 731 & 2193 & 51.1 & 55.0 [35.0–69.0] \\
CC & 2020 & 6060 & 49.5 & 55.0 [35.0–70.0] \\
EC & 529 & 1587 & 55.1 & 56.0 [36.0–69.0] \\
LC & 1771 & 5313 & 53.3 & 55.0 [34.0–68.0] \\
PC & 1958 & 5874 & 48.6 & 56.0 [34.0–70.0] \\
\bottomrule
\end{tabularx}
\end{table}
We constructed case--control cohorts for GC (target) and four source cancers (CC, EC, LC, PC) at a 1:3 ratio.
Table~\ref{tab:cohort} summarizes sample sizes and demographics.
Across cohorts, the sex distribution was approximately balanced (overall $\sim$51\% male) and the age distribution was comparable, with medians centered near 55--56 years and interquartile ranges spanning the mid-30s to around 70 years. 
Within the pretraining pool, EC constitutes the smallest cohort (529 cases, 1{,}587 controls), whereas CC and PC provide the largest case counts (2{,}020 and 1{,}958, respectively), with LC intermediate (1{,}771). 
In aggregate, the non-GC sources contribute 6{,}278 cases and 18{,}834 controls, compared with 731 and 2{,}193 for GC; thus, the pretraining pool supplies $\sim$8.6$\times$ more labeled samples than the GC task alone. 
This size differential is pertinent for transfer learning, as it enables the backbone to learn cancer-agnostic structure from richer supervision before adaptation to GC.



\subsection{Comparison of Models}
%\input{tex_and_figs/table_model_compare_gc1.tex}
\begin{table}[htbp]
\caption{Performance of models.\label{tab:model-compare-gc1}}
%\begin{adjustwidth}{-\extralength}{0cm}
\begin{tabularx}{\textwidth}{CP{3cm}CCCC}
\toprule
Model & AUROC (CI 95 \%) & AP & Sensitivity & Specificity & F1 \\
\midrule
LR & 0.921 (0.916--0.926) & 0.780 & 0.843 & 0.844 & 0.731 \\
XGB & 0.928 (0.923--0.933) & 0.806 & 0.865 & 0.822 & 0.723 \\
MLP & 0.930 (0.925--0.934) & 0.811 & 0.881 & 0.827 & 0.735 \\
Transfer & 0.938 (0.933--0.943) & 0.826 & 0.890 & 0.838 & 0.751 \\
\bottomrule
\end{tabularx}
%\end{adjustwidth}
\end{table}
As summarized in Table~\ref{tab:model-compare-gc1}, the Transfer model achieved the highest discrimination and precision–recall performance, outperforming strong non-transfer baselines.
The Transfer model achieved the best overall discrimination (AUROC $0.938$, 95\% CI $0.933$–$0.943$) and precision--recall performance (AP $0.826$), exceeding the non-transfer MLP trained from scratch (AUROC $0.930$ [0.925–0.934], AP $0.811$), XGB (AUROC $0.928$, AP $0.806$), and LR (AUROC $0.921$, AP $0.780$).
Relative to the scratch MLP, transfer learning improved AUROC by $+0.008$ and AP by $+0.015$, with a favorable operating profile (sensitivity $0.890$, specificity $0.838$, $F_1=0.751$). These gains suggest that pretraining on related non-GC cancers yields more informative representations for GC risk, translating into small but consistent improvements over the baselines.


\subsection{Model performance under reduced GC training labels}
\begin{figure}[htbp]
\centering
\subfloat[AUROC]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_model_gc_rate_auroc.pdf}}
\subfloat[AP]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_model_gc_rate_ap.pdf}} \\
%\subfloat[Sensitivity]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_model_gc_rate_sensitivity.pdf}}
%\subfloat[Specificity]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_model_gc_rate_specificity.pdf}} \\
\subfloat[F1]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_model_gc_rate_f1.pdf}}
\caption{Performance across the GC sampling rate $r$. Lower $r$ simulates fewer labeled GC \emph{training} cases.\label{fig:model-gcrate}}
\end{figure}
As summarized in Figure~\ref{fig:model-gcrate} (AUROC, AP, and F1 panels), we varied the \emph{GC sampling rate} $r \in \{1.00, 0.50, 0.20, 0.10, 0.05, 0.02, 0.01\}$ to quantify robustness under limited GC labels.
For each $r$, we applied stratified sampling at proportion $r$ on the GC training partition (preserving the case{:}control ratio), while keeping the GC validation and test partitions fixed.
Non-GC source datasets for pretraining and all preprocessing were unchanged across $r$.
All models (LR, XGB, MLP-from-scratch, and Transfer) were then trained on the resulting $r$-subsampled GC training set for that rate.
The Transfer model always used the same pretraining configuration on non-GC cancers and was fine-tuned on the $r$-specific GC training set.

As $r$ decreases, all models show the expected degradation in AUROC, AP, and F1 due to reduced sample size (see Figure~\ref{fig:model-gcrate}).
Across all rates, the Transfer model maintains the best discrimination and precision--recall performance, with the absolute gains most pronounced at the smallest $r$.
These findings indicate that pretraining on related non-GC cancers consistently improves GC risk modeling across a wide range of labeled-data regimes, with the largest improvements in AP and F1 under the scarcest-label settings.
%At \texttt{gc\_rate}$=0.01$, Transfer achieves AUROC~0.916 and AP~0.765, surpassing the scratch MLP (AUROC~0.813, AP~0.546) by $+0.103$ AUROC and $+0.219$ AP, and yielding the highest F1 (0.715 vs.\ 0.600; $+0.115$).
%The advantage remains substantial at \texttt{gc\_rate}$=0.02$ (AUROC $+0.075$, AP $+0.165$, F1 $+0.088$) and \texttt{gc\_rate}$=0.05$ (AUROC $+0.058$, AP $+0.133$, F1 $+0.085$). 

%With more abundant labels, the gap narrows but persists.
%At \texttt{gc\_rate}$=0.10$, Transfer improves over MLP by $+0.040$ AUROC, $+0.097$ AP, and $+0.048$ F1; at \texttt{gc\_rate}$=0.50$, the corresponding margins are $+0.013$, $+0.027$, and $+0.027$; and at \texttt{gc\_rate}$=1.00$, Transfer still leads (AUROC 0.938 vs.\ 0.930; AP 0.826 vs.\ 0.811; F1 0.751 vs.\ 0.735).
%Notably, AUROC is relatively robust to prevalence, yet non-transfer models degrade markedly at \texttt{gc\_rate}$\le 0.05$, whereas Transfer retains higher AUROC (0.916--0.924) and substantially higher AP (0.765--0.787). 



%\begin{table}[t]
%\centering
%\small
%\begin{tabular}{lllllll}
%\toprule
%gc\textbackslash \_rate & Model & AUROC & AP & Sensitivity & Specificity & F1 \\
%\midrule
%0.01 & LR & 0.814 & 0.561 & 0.770 & 0.730 & 0.599 \\
%0.01 & XGB & 0.799 & 0.528 & 0.766 & 0.714 & 0.586 \\
%0.01 & MLP & 0.813 & 0.546 & 0.787 & 0.719 & 0.600 \\
%0.01 & Transfer & 0.916 & 0.765 & 0.865 & 0.814 & 0.715 \\
%0.02 & LR & 0.837 & 0.577 & 0.809 & 0.753 & 0.635 \\
%0.02 & XGB & 0.819 & 0.564 & 0.751 & 0.740 & 0.595 \\
%0.02 & MLP & 0.845 & 0.608 & 0.775 & 0.781 & 0.638 \\
%0.02 & Transfer & 0.920 & 0.773 & 0.852 & 0.834 & 0.726 \\
%0.05 & LR & 0.867 & 0.634 & 0.811 & 0.780 & 0.657 \\
%0.05 & XGB & 0.850 & 0.620 & 0.783 & 0.755 & 0.622 \\
%0.05 & MLP & 0.866 & 0.654 & 0.798 & 0.774 & 0.645 \\
%0.05 & Transfer & 0.924 & 0.787 & 0.864 & 0.831 & 0.730 \\
%0.10 & LR & 0.885 & 0.665 & 0.827 & 0.790 & 0.674 \\
%0.10 & XGB & 0.881 & 0.693 & 0.807 & 0.789 & 0.662 \\
%0.10 & MLP & 0.886 & 0.693 & 0.859 & 0.779 & 0.682 \\
%0.10 & Transfer & 0.926 & 0.790 & 0.838 & 0.847 & 0.730 \\
%0.20 & LR & 0.901 & 0.718 & 0.855 & 0.803 & 0.700 \\
%0.20 & XGB & 0.899 & 0.725 & 0.830 & 0.809 & 0.691 \\
%0.20 & MLP & 0.908 & 0.757 & 0.847 & 0.821 & 0.712 \\
%0.20 & Transfer & 0.928 & 0.799 & 0.858 & 0.836 & 0.732 \\
%0.50 & LR & 0.914 & 0.768 & 0.871 & 0.821 & 0.724 \\
%0.50 & XGB & 0.917 & 0.785 & 0.825 & 0.832 & 0.708 \\
%0.50 & MLP & 0.922 & 0.785 & 0.870 & 0.819 & 0.722 \\
%0.50 & Transfer & 0.935 & 0.812 & 0.871 & 0.847 & 0.749 \\
%1.00 & LR & 0.921 & 0.780 & 0.843 & 0.844 & 0.731 \\
%1.00 & XGB & 0.928 & 0.806 & 0.865 & 0.822 & 0.723 \\
%1.00 & MLP & 0.930 & 0.811 & 0.881 & 0.827 & 0.735 \\
%1.00 & Transfer & 0.938 & 0.826 & 0.890 & 0.838 & 0.751 \\
%\bottomrule
%\end{tabular}
%
%\caption{Performance across $\textbackslash\{\}mathrm\{gc\textbackslash\{\}\_rate\}\textbackslash\{\}in\textbackslash\{\}\{1.0,0.5,0.2,0.1,0.05\textbackslash\{\}\}$ for each model. Transfer uses Main constraints at each rate.}
%\label{tab:model-gc-rate-sweep}
%\end{table}


\subsection{Effect of pretraining source composition}
%\input{tex_and_figs/table_transfer_by_pretrain_types.tex}
\begin{table}[htbp]
\caption{The performance of \emph{Transfer} grouped by pre-train cancer types.\label{tab:transfer-by-pretrain-types}}
\begin{tabularx}{\textwidth}{P{3cm}CCCCC}
\toprule
Pretrain types & AUROC & AP & Sensitivity & Specificity & F1 \\
\midrule
LC & 0.931 & 0.809 & 0.875 & 0.839 & 0.743 \\
EC & 0.932 & 0.811 & 0.878 & 0.841 & 0.747 \\
CC & 0.932 & 0.815 & 0.866 & 0.849 & 0.748 \\
PC & 0.934 & 0.816 & 0.873 & 0.848 & 0.750 \\
CC+EC+LC+PC & 0.938 & 0.826 & 0.890 & 0.838 & 0.751 \\
\bottomrule
\end{tabularx}
\end{table}
To examine how pretraining source composition influences transfer performance, 
we repeated pretraining using one source cancer at a time and compared the results 
to the model pretrained on all four non-GC cancers (CC, EC, LC, PC). 
As shown in Table~\ref{tab:transfer-by-pretrain-types}, 
every single-source model outperformed or matched the non-transfer MLP baseline 
(Table~\ref{tab:model-compare-gc1}), with AUROC values clustered within 0.931–0.934 
and AP within 0.809–0.816. 
Among single sources, PC yielded the highest AUROC (0.934) and F1 (0.750), 
while CC achieved the strongest AP (0.815). 
EC and LC were comparable, with EC slightly higher in sensitivity and LC slightly higher in specificity.

Pretraining on all four cancers (CC+EC+LC+PC) achieved the numerically best overall results 
(AUROC 0.938, AP 0.826, F1 0.751), but the difference from single-source variants was modest. 
Thus, while the multi-source model performed best in this experiment, 
we do not claim that this combination is universally optimal. 
Instead, these findings suggest that leveraging multiple heterogeneous source cancers 
can provide complementary EHR-visible signals, 
and that carefully balancing or selecting diverse sources 
offers potential to further improve generalization for gastric cancer prediction.


\begin{figure}
\centering
\subfloat[AUROC]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_transfer_combo_gc_rate_auroc.pdf}}
\subfloat[AP]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_transfer_combo_gc_rate_ap.pdf}}\\
\subfloat[F1]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_transfer_combo_gc_rate_f1.pdf}}
\caption{Effect of \texttt{ft\_head\_only}/\texttt{sp\_reg} across gc\_rate under head\_mode=new and CC+EC+LC+PC pretraining. Lines: main (True/False), False/True, False/False.}
\label{fig:transfer-combo-gcrate}
\end{figure}


%\subsection{Primary Test Performance}
%We compare LR, XGB, and single-task MLP trained on GC only with the proposed Transfer-MLP (pretrained on CC/EC/LC/PC, fine-tuned on GC).
%Performance is reported as AUROC and AP on the held-out test set;
%Sensitivity, Specificity, and F1 are computed at the validation-selected threshold $t^\star$ that maximizes F1. % (Code refs: Train_MLP/LR/XGB, get_best_f1_threshold)
%% :contentReference[oaicite:32]{index=32} :contentReference[oaicite:33]{index=33} :contentReference[oaicite:34]{index=34}
%We show mean $\pm$ SD over five seeds and 95\% CIs from patient-level bootstrap.
%Table~\ref{tab:main} presents the results.

\subsection{Head-only vs.\ Full-model Fine-tuning (Freezing the Backbone)}
\begin{table}[t]
\centering
\small
\begin{tabular}{llrrrrr}
\toprule
gc\textbackslash \_rate & Variant & AUROC & AP & Sensitivity & Specificity & F1 \\
\midrule
0.01 & FT-Full & 0.915000 & 0.768000 & 0.842000 & 0.829000 & 0.715000 \\
0.01 & FT-Head & 0.916000 & 0.765000 & 0.865000 & 0.814000 & 0.715000 \\
0.02 & FT-Full & 0.918000 & 0.773000 & 0.835000 & 0.840000 & 0.722000 \\
0.02 & FT-Head & 0.920000 & 0.773000 & 0.852000 & 0.834000 & 0.726000 \\
0.05 & FT-Full & 0.924000 & 0.790000 & 0.851000 & 0.834000 & 0.725000 \\
0.05 & FT-Head & 0.924000 & 0.787000 & 0.864000 & 0.831000 & 0.730000 \\
0.10 & FT-Full & 0.924000 & 0.785000 & 0.851000 & 0.830000 & 0.722000 \\
0.10 & FT-Head & 0.926000 & 0.790000 & 0.838000 & 0.847000 & 0.730000 \\
0.20 & FT-Full & 0.927000 & 0.791000 & 0.853000 & 0.839000 & 0.731000 \\
0.20 & FT-Head & 0.928000 & 0.799000 & 0.858000 & 0.836000 & 0.732000 \\
0.50 & FT-Full & 0.929000 & 0.797000 & 0.868000 & 0.827000 & 0.728000 \\
0.50 & FT-Head & 0.935000 & 0.812000 & 0.871000 & 0.847000 & 0.749000 \\
1.00 & FT-Full & 0.930000 & 0.800000 & 0.849000 & 0.844000 & 0.734000 \\
1.00 & FT-Head & 0.938000 & 0.826000 & 0.890000 & 0.838000 & 0.751000 \\
\bottomrule
\end{tabular}
\caption{Test performance of \textbackslash\{\}texttt\{Transfer\} variants across $\textbackslash\{\}mathrm\{gc\textbackslash\{\}\_rate\}$ (best-by-validation at each rate within each variant).}
\label{tab:transfer-combo-gc-rate}
\end{table}
\label{subsec:freeze-ablation}
\textbf{Experimental setup.}
To isolate the effect of freezing the pretrained encoder during GC adaptation, we compare two transfer variants across GC label budgets $r\in\{0.01,0.02,0.05,0.10,0.20,0.50,1.00\}$:
(i) \emph{FT-Head} (head-only fine-tuning), which \emph{freezes} the MLP backbone pretrained on CC+EC+LC+PC and trains only a new GC head; and
(ii) \emph{FT-Full} (full-model fine-tuning), which \emph{updates} both the pretrained backbone and the GC head.
For every $r$, we keep the GC \emph{validation} and \emph{test} partitions fixed, subsample only the GC \emph{training} partition at rate $r$ (preserving the 1{:}3 case:control ratio), and retain the same non-GC pretraining data and preprocessing.
Within each variant and each $r$, we train with the same objective, class weighting, and regularization as in Section~\ref{sec:methods}; model selection is performed by mean validation AUROC (best-by-validation), and the table reports test metrics for the selected setting (aggregated as described in \S\ref{sec:analysis}).
Table~\ref{tab:transfer-combo-gc-rate} summarizes the results.

As shown in Table~\ref{tab:transfer-combo-gc-rate}, \textbf{head-only fine-tuning (FT-Head)} consistently matches or exceeds \textbf{full-model fine-tuning (FT-Full)} across GC label budgets. Under the scarcest labels ($r{=}0.01$–$0.02$), the two variants are effectively tied in AUROC and AP (e.g., at $r{=}0.02$, AUROC $0.920$ vs.\ $0.918$ and AP $0.773$ vs.\ $0.773$), with FT-Head yielding a small F1 edge (0.726 vs.\ 0.722). As $r$ increases, FT-Head obtains clearer gains: at $r{=}0.10$, AUROC/AP/F1 improve from $0.924/0.785/0.722$ (FT-Full) to $0.926/0.790/0.730$ (FT-Head); at $r{=}0.50$, the margin widens to $0.935/0.812/0.749$ vs.\ $0.929/0.797/0.728$; and at $r{=}1.00$, FT-Head attains our best overall performance (AUROC $0.938$, AP $0.826$, F1 $0.751$).

These results suggest that freezing the pretrained backbone while adapting only the prediction head preserves cross-cancer structure learned during pretraining and reduces variance in the GC adaptation step—benefits that persist even when more GC labels are available. In contrast, updating all layers can slightly overfit tabular EHR signals in our setting, narrowing but not eliminating the gap as $r$ grows. This behavior is consistent with prior observations that shared representations learned on related tasks transfer effectively and that restricting adaptation capacity can improve sample efficiency and stability~\citep{Caruana1997_MTL,Pan2010_TLsurvey,Yosinski2014_transferable}.



\subsection{Ablation Studies}
\paragraph{Source composition and head strategies.}
We varied pretraining sources (single vs. multiple) and head strategies (avg/new).
% Multi-type batching and per-type validation selection are implemented as in Transfer\_MLP. 
% :contentReference[oaicite:35]{index=35}
Table~\ref{tab:ablation} shows GC test AUROC/AP and the deltas vs.\ single-task MLP.


\paragraph{Data efficiency.}
We reduced the GC training fraction ($\texttt{gc\_rate}\in\{1.0,0.5,0.2,0.1,0.05\}$) and observed Transfer vs.\ single-task performance. % (Code refs: settings.py, run\_single\_experiment.py)
% :contentReference[oaicite:36]{index=36} :contentReference[oaicite:37]{index=37}

\subsection{Threshold-dependent Clinical Metrics}
At $t^\star$, we report Sensitivity, Specificity, and F1, and additionally PPV/NPV (with 95\% CIs).
% Threshold selection: get\_best\_f1\_threshold; metric computation: get\_sens\_spec\_f1.
% :contentReference[oaicite:38]{index=38}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

Authors should discuss the results and how they can be interpreted from the perspective of previous studies and of the working hypotheses. The findings and their implications should be discussed in the broadest context possible. Future research directions may also be highlighted.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

This section is not mandatory, but can be added to the manuscript if the discussion is unusually long or complex.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following supporting information can be downloaded at:  \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.}

% Only for journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title. A supporting video article is available at doi: link.}

% Only used for preprtints:
% \supplementary{The following supporting information can be downloaded at the website of this paper posted on \href{https://www.preprints.org/}{Preprints.org}.}

% Only for journal Hardware:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.\vspace{6pt}\\
%\begin{tabularx}{\textwidth}{lll}
%\toprule
%\textbf{Name} & \textbf{Type} & \textbf{Description} \\
%\midrule
%S1 & Python script (.py) & Script of python source code used in XX \\
%S2 & Text (.txt) & Script of modelling code used to make Figure X \\
%S3 & Text (.txt) & Raw data from experiment X \\
%S4 & Video (.mp4) & Video demonstrating the hardware in use \\
%... & ... & ... \\
%\bottomrule
%\end{tabularx}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization, D.H.; methodology, D.H., J.K. and J.J.; software, D.H., J.K. and J.J.; formal analysis, D.H., J.K. and J.J.; investigation, D.H., J.K. and J.J.; resources, D.H.; data curation, D.H., J.K. and J.J.; writing---original draft, D.H.; supervision, D.H.
All authors have read and agreed to the published version of the manuscript.}

\funding{This study was supported by the National R\&D Program for Cancer Control through the National Cancer Center(NCC) funded by the Ministry of Health \& Welfare, Republic of Korea(RS-2025-02264000).
This work was also supported by 2024 Research Fund of Myongji University.}

\institutionalreview{Ethical review and approval were waived for this study by the Institutional Review Board of Myongji University (IRB No. MJU 2025-09-001, approval date: 23 September 2025), because the research used only publicly available data (MIMIC-IV database) and did not involve the direct recruitment of human participants. The study was conducted in accordance with the Declaration of Helsinki.}

\informedconsent{Patient consent was waived because the study was an analysis of a third-party anonymized publicly available database.}

\dataavailability{
The datasets in this study are available through the MIMIC-IV database at \url{https://physionet.org/content/mimiciv/3.1/} (accessed on 1 October 2025) with the permission of PhysioNet.}

% Only for journal Drones
%\durcstatement{Current research is limited to the [please insert a specific academic field, e.g., XXX], which is beneficial [share benefits and/or primary use] and does not pose a threat to public health or national security. Authors acknowledge the dual-use potential of the research involving xxx and confirm that all necessary precautions have been taken to prevent potential misuse. As an ethical responsibility, authors strictly adhere to relevant national and international laws about DURC. Authors advocate for responsible deployment, ethical considerations, regulatory compliance, and transparent reporting to mitigate misuse risks and foster beneficial outcomes.}

% Only for journal Nursing Reports
%\publicinvolvement{Please describe how the public (patients, consumers, carers) were involved in the research. Consider reporting against the GRIPP2 (Guidance for Reporting Involvement of Patients and the Public) checklist. If the public were not involved in any aspect of the research add: ``No public involvement in any aspect of this research''.}
%
%% Only for journal Nursing Reports
%\guidelinesstandards{Please add a statement indicating which reporting guideline was used when drafting the report. For example, ``This manuscript was drafted against the XXX (the full name of reporting guidelines and citation) for XXX (type of research) research''. A complete list of reporting guidelines can be accessed via the equator network: \url{https://www.equator-network.org/}.}
%
%% Only for journal Nursing Reports
%\useofartificialintelligence{Please describe in detail any and all uses of artificial intelligence (AI) or AI-assisted tools used in the preparation of the manuscript. This may include, but is not limited to, language translation, language editing and grammar, or generating text. Alternatively, please state that “AI or AI-assisted tools were not used in drafting any aspect of this manuscript”.}

\conflictsofinterest{The authors declare no conflicts of interest.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional

%% Only for journal Encyclopedia
%\entrylink{The Link to this entry published on the encyclopedia platform.}

\abbreviations{Abbreviations}{
The following abbreviations are used in this manuscript:
\\

\noindent 
\begin{tabular}{@{}ll}
GC & Gastric cancer\\
CC & Colorectal cancer\\
EC & Esophageal cancer\\
LC & Liver cancer\\
PC & Pancreatic cancer
%MDPI & Multidisciplinary Digital Publishing Institute\\
%DOAJ & Directory of open access journals\\
%TLA & Three letter acronym\\
%LD & Linear dichroism
\end{tabular}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
\appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixstart
\appendix
\section[\appendixname~\thesection]{}
\subsection[\appendixname~\thesubsection]{}
The appendix is an optional section that can contain details and data supplemental to the main text---for example, explanations of experimental details that would disrupt the flow of the main text but nonetheless remain crucial to understanding and reproducing the research shown; figures of replicates for experiments of which representative data are shown in the main text can be added here if brief, or as Supplementary Data. Mathematical proofs of results not central to the paper can be added as an appendix.

\begin{table}[H] 
\caption{This is a table caption.\label{tab5}}
%\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{CCC}
\toprule
\textbf{Title 1}	& \textbf{Title 2}	& \textbf{Title 3}\\
\midrule
Entry 1		& Data			& Data\\
Entry 2		& Data			& Data\\
\bottomrule
\end{tabularx}
\end{table}

\section[\appendixname~\thesection]{}
All appendix sections must be cited in the main text. In the appendices, Figures, Tables, etc. should be labeled, starting with ``A''---e.g., Figure A1, Figure A2, etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\isPreprints{}{% This command is only used for ``preprints''.
\begin{adjustwidth}{-\extralength}{0cm}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
\bibliography{references}


% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
%\isPreprints{}{% This command is only used for ``preprints''.
\end{adjustwidth}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
\end{document}

