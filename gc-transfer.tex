%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[diagnostics,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aieduc, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, amh, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, glacies, grasses, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, iic, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdad, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joitmc, joma, jop, jor, journalmedia, jox, jpbi, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidney, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, benchmark, book, bookreview, briefcommunication, briefreport, casereport, changes, clinicopathologicalchallenge, comment, commentary, communication, conceptpaper, conferenceproceedings, correction, conferencereport, creative, datadescriptor, discussion, entry, expressionofconcern, extendedabstract, editorial, essay, erratum, fieldguide, hypothesis, interestingimages, letter, meetingreport, monograph, newbookreceived, obituary, opinion, proceedingpaper, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, supfile, systematicreview, technicalnote, viewpoint, guidelines, registeredreport, tutorial,  giantsinurology, urologyaroundtheworld
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}
%\usepackage{kotex}

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Cross-Cancer Transfer Learning for Gastric Cancer Risk Prediction from Electronic Health Records}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Cross-Cancer Transfer Learning for Gastric Cancer Risk Prediction from Electronic Health Records}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0003-3002-1972} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-0000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Daeyoung Hong $^{1,}$*\orcidA{}, Jiung Kim $^{1}$ and Jiyong Jung $^{1}$}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Daeyoung Hong, Jiung Kim and Jiyong Jung}

% MDPI internal command: Authors, for citation in the left column, only choose below one of them according to the journal style
% If this is a Chicago style journal 
% (arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci): 
% Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% If this is a APA style journal 
% (admsci, behavsci, businesses, econometrics, economies, education, ejihpe, games, humans, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth): 
% Lastname, F., Lastname, F., \& Lastname, F.

% If this is a ACS style journal (Except for the above Chicago and APA journals, all others are in the ACS format): 
% Lastname, F.; Lastname, F.; Lastname, F.
\isAPAStyle{%
       \AuthorCitation{Hong, D., Kim, J., \& Jung, J.}
         }{%
        \isChicagoStyle{%
        \AuthorCitation{Hong, Daeyoung, Jiung Kim, and Jiyong Jung.}
        }{
        \AuthorCitation{Hong, D.; Kim, J.; Jung, J.}
        }
}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address[1]{%
School of Software Convergence, Myongji University, Seoul 03674, Republic of Korea; jiung8758@gmail.com (J.K.); jiyongj954@gmail.com (J.J.)}
%\\$^{2}$ \quad Affiliation 2; e-mail@e-mail.com}

% Contact information of the corresponding author
\corres{Correspondence: dyhong@mju.ac.kr}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation.}  
% Current address should not be the same as any items in the Affiliation section.

%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes.

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{\noindent\textbf{Background:}
Timely identification of individuals at elevated risk for gastric cancer (GC) within routine care could enable earlier endoscopy and referral.
We posit that cancers within the gastrointestinal/hepatopancreatobiliary spectrum share signals that can be leveraged via transfer learning on electronic health records (EHRs) variables.
%Early identification of gastric cancer (GC) risk from electronic health records (EHRs) could enable timelier endoscopy.
\textbf{Methods:}
We developed a cross-cancer transfer learning framework (Transfer) that pre-trains a multilayer perceptron (MLP) on non-GC gastrointestinal/hepatopancreatobiliary cancers (colorectal, esophageal, liver, pancreatic) and then adapts to GC using only structured variables from the de-identified MIMIC-IV v3.1 database. We compared transfer variants against strong non-transfer baselines (logistic regression, XGBoost, architecturally matched MLP) across decreasing GC label fractions to mimic data-scarce regimes. Primary endpoints were AUROC and average precision (AP); secondary endpoints included F1 and sensitivity/specificity.
\textbf{Results:}
In the full-label setting, Transfer achieved AUROC $0.854$ and AP $0.600$, outperforming logistic regression (LR), extreme gradient boosting (XGB) and improving over the scratch MLP in AUROC ($+0.024$) and F1 ($+0.027$), while AP was essentially tied (Transfer $0.600$ vs.\ MLP $0.603$).
As GC labels were reduced, Transfer maintained the strongest overall performance.
\textbf{Conclusions:}
%Cross-cancer transfer on EHRs suggests a potentially sample-efficient strategy to GC risk modeling—facilitating scalable deployment and helping prioritize endoscopic evaluation within routine workflows. By reusing related cancer cohorts to bootstrap GC representation learning, the approach strengthens early-risk identification under label scarcity and supports clinician decision-making at the point of care.
\textcolor{red}{
Cross-cancer transfer on EHRs suggests a sample-efficient, deployment-ready strategy for GC risk modeling, particularly for health systems with limited GC labels but existing GI/HPB cancer cohorts, and may help prioritize endoscopic evaluation and follow-up within routine workflows. By reusing related cancer cohorts to bootstrap GC representation learning, the approach strengthens early-risk identification under label scarcity and supports clinician decision-making at the point of care.
}
}
%Cross-cancer transfer on structured EHRs offers a generalizable, sample-efficient approach to GC risk modeling based on electronic health records (EHRs)}

% Keywords
\keyword{gastric cancer; electronic health records; prediction model; machine learning; transfer learning} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal BioTech, Fishes, Neuroimaging and Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Advances in Respiratory Medicine, Future, Sensors and Smart Cities
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%
%
%\noindent This is an obligatory section in ``Advances in Respiratory Medicine'', ``Future'', ``Sensors'' and ``Smart Cities”, whose goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\begin{document}
\newcommand{\GC}{\mathrm{GC}}
\newcommand{\CC}{\mathrm{CC}}
\newcommand{\EC}{\mathrm{EC}}
\newcommand{\LC}{\mathrm{LC}}
\newcommand{\PC}{\mathrm{PC}}
\newcommand{\encP}{\phi}  % encoder parameters
\newcommand{\enc}[1]{f_{#1}}
\newcommand{\headP}[1]{\theta_{#1}}
\newcommand{\head}[1]{h_{\headP{#1}}}
\newcommand{\pretrainP}{\encP^\star}
\newcommand{\preDecayC}{\lambda_{\mathrm{pre}}}  % pretraining decay coefficient
\newcommand{\ftDecayC}{\lambda_{\mathrm{ft}}}  % FT decay coefficient
\section{Introduction}
Gastric cancer (GC) continues to pose a major challenge to global health, 
ranking as the fifth most frequently diagnosed malignancy and the fourth leading cause of cancer-related death worldwide~\citep{sung2021global}. 
Prognosis remains strongly dependent on the stage at diagnosis, 
with 5-year survival rates exceeding 70\% for localized disease but dropping below 10\% for metastatic GC~\citep{NCI2023stomach}.

Harnessing routinely collected electronic health record (EHR) data for earlier GC identification could improve outcomes by enabling timely endoscopy and treatment.
Structured EHR signals—such as demographics, diagnoses, and laboratory data—represent an attractive source for data-driven cancer screening~\citep{read2023cancers,huang2022jco}.
However, EHR-based model development can face persistent challenges, including incomplete or missing records, irregular sampling intervals, and the overall complexity of clinical documentation.
Moreover, ethical and regulatory requirements—such as institutional approvals or patient consent—can further limit the volume of analyzable data, 
leading to relatively small, heterogeneous datasets for model training.
% TODO: citation

For gastric cancer prediction, machine learning models such as logistic regression and XGBoost, using routine structured variables (e.g., demographics, diagnoses, and common laboratory tests), have been investigated~\citep{park2024SHapley,huang2022jco,Kim2024EHRGC}.
Despite these advances, most prior studies have focused on within-cancer prediction and have not examined whether cross-cancer regularities in structured EHR signals (e.g., anemia, inflammation, and metabolic axes) can be distilled and transferred to improve GC modeling under data scarcity.

%Across gastrointestinal (GI) and hepatopancreatobiliary (HPB) malignancies, multiple EHR-visible laboratory and clinical signatures overlap with those leveraged for GC detection. 
%Luminal GI tract tumors often cause gradual occult bleeding that culminates in iron-deficiency anemia (IDA), producing characteristic shifts in longitudinal complete blood count (CBC) measures~\citep{read2023cancers,Aksoy2019CBC,Krieg2024IDA,Kim2014GIBleed}. 
%In liver disease and primary liver cancer, anemia and systemic inflammation/nutritional markers (e.g., albumin, C-reactive protein, neutrophil–lymphocyte and platelet–lymphocyte ratios) are prominent and are also established biomarkers of GC prognosis~\citep{Stein2016Anemia,Gkamprela2017CLDIDA,Crumley2010Albumin,Kim2020Inflam}. 
%Pancreatic cancer frequently perturbs glucose metabolism; dysglycemia and diabetes are associated with GC risk, suggesting that encoders exposed to glucose/HbA1c dynamics may learn representations transferable to GC without implying causation~\citep{Shimoyama2013DMGC,Yoon2013DMGC,Guo2022DMGC}. 
%These cross-cancer regularities motivate leveraging non-GC cohorts to improve sample efficiency for GC modeling.

{\color{red}
Across gastrointestinal (GI) and hepatopancreatobiliary (HPB) malignancies, several clinical and laboratory signatures that are visible in structured EHRs recur across tumor types and are directly relevant for GC. Luminal GI tumors, particularly colorectal and gastric cancers, often present with chronic occult bleeding and iron-deficiency anemia (IDA); population-based studies using primary care or administrative data show that anemia and low hemoglobin are strong early signals for colorectal cancer, and clinical practice updates emphasize that otherwise unexplained IDA in adults should prompt evaluation of the upper and lower GI tract for malignancy~\citep{Hamilton2008CRC,Aksoy2019CBC,Krieg2024IDA,AGA2024IDA}. In chronic liver disease and primary liver cancer, IDA frequently co-occurs with systemic inflammation and malnutrition, and biomarkers such as albumin, C-reactive protein, neutrophil–lymphocyte ratio, and platelet–lymphocyte ratio are prognostic in both liver cancer and GC~\citep{Stein2016Anemia,Gkamprela2017CLDIDA,Crumley2010Albumin,Kim2020Inflam}. Metabolic syndrome and dysglycemia form another shared axis: large cohort and meta-analytic studies link diabetes and metabolic syndrome not only to colorectal and pancreatic cancer risk but also to increased GC incidence~\citep{MetS_CRC_2021,Zhong2023MetS_PC,Shimoyama2013DMGC,Yoon2013DMGC,Guo2022DMGC}. Taken together, these data suggest that routinely collected features such as complete blood count indices, basic chemistries, and ICD-derived codes for IDA, liver disease, diabetes, and other metabolic comorbidities encode biologically meaningful cross-cancer information. A multi-cancer pretraining stage can therefore exploit this shared structure to learn representations that are transferable to GC.
}


%Across gastrointestinal (GI) and hepatopancreatobiliary (HPB) malignancies, multiple EHR-visible laboratory and clinical signatures overlap with those leveraged for GC detection. 
%{
%Luminal GI tract tumors often cause gradual occult bleeding that culminates in iron-deficiency anemia (IDA), producing characteristic shifts in longitudinal complete blood count (CBC) measures~\citep{read2023cancers,Aksoy2019CBC,Krieg2024IDA,Kim2014GIBleed,Hamilton2008CRC,AGA2024IDA}. 
%In liver disease and primary liver cancer, anemia and systemic inflammation/nutritional markers (e.g., albumin, C-reactive protein, neutrophil–lymphocyte and platelet–lymphocyte ratios) are prominent and are also established biomarkers of GC prognosis~\citep{Stein2016Anemia,Gkamprela2017CLDIDA,Crumley2010Albumin,Kim2020Inflam}. 
%Pancreatic cancer frequently perturbs glucose metabolism; dysglycemia and diabetes are associated with increased GC risk, suggesting that encoders exposed to glucose/HbA1c dynamics may learn representations transferable to GC without implying causation~\citep{Shimoyama2013DMGC,Yoon2013DMGC,Guo2022DMGC}. 
%Beyond GC, metabolic syndrome and its components (central obesity, insulin resistance, dyslipidemia) are associated with elevated risks of colorectal and pancreatic cancer, reinforcing the role of shared metabolic axes across our source cancers~\citep{MetS_CRC_2021,Zhong2023MetS_PC}. 
%Taken together, these cross-cancer regularities in anemia, inflammation, nutritional status, and metabolic disturbance provide a clinically grounded rationale for using non-GC GI/HPB cohorts as source tasks in our multi-source transfer framework, rather than treating them as abstract ``overlapping information''.
%}

%{
%Beyond these general overlaps, several specific clinical and biological axes further motivate multi-source transfer.
%First, chronic occult gastrointestinal blood loss and iron-deficiency anemia are well-recognized alarm features for colorectal and upper GI malignancy and are accompanied by characteristic changes in hemoglobin and red cell indices that are captured in routine CBC testing~\citep{Hamilton2008CRC,Krieg2024IDA,AGA2024IDA}.
%Second, cardiometabolic traits that define metabolic syndrome---including central obesity, dyslipidemia, hypertension, and type 2 diabetes---have been associated with increased risks of colorectal, pancreatic, and gastric cancers~\citep{MetS_CRC_2021,Zhong2023MetS_PC,Shimoyama2013DMGC,Yoon2013DMGC,Guo2022DMGC}.
%These processes manifest in the same comorbidities and laboratory panels (CBC/CMP, glucose, lipids, blood pressure) that we use as structured EHR features.
%Taken together, these shared pathophysiologic mechanisms provide a clinical rationale for viewing non-GC GI/HPB cancers as biologically plausible source tasks for GC, rather than simply as datasets with ``overlapping information'' in feature space.
%}

Deep learning (DL) provides a principled way to learn non-linear interactions from tabular EHR variables and to distill noisy laboratory measurements into task-relevant representations. 
When labels for GC are limited, transfer learning and multi-task learning can mitigate data scarcity by first inducing shared, cancer-agnostic representations on related cancers and then adapting them to GC~\citep{Caruana1997_MTL,Pan2010_TLsurvey,Yosinski2014_transferable}. 
In this paradigm, DL primarily functions as a representation learner that captures cross-cancer laboratory signatures expected to recur in GC, thereby improving sample efficiency and stability during adaptation.

\textcolor{red}{
Recent work has increasingly explored self-supervised pretraining and EHR foundation models, which learn general-purpose patient representations using contrastive or masked-feature objectives before task-specific fine-tuning~\citep{Xia2024ContrastiveEHR,Mataraso2025EHRFM,Guo2023SharedEHRFM}.
Our work is complementary to these efforts: instead of learning a general-purpose foundation model from very large unlabeled corpora, we study a supervised transfer setting in which labeled non-GC cancer cohorts are used to pre-train a neural encoder and then adapt it to GC.
This design examines how effectively structured variables and cross-cancer label supervision can be leveraged for GC representation learning, while remaining compatible with future integration of self-supervised objectives.
}

\textcolor{red}{
We focus our initial modeling on routinely collected structured EHR variables (demographics, ICD-derived comorbidities, and routine laboratory tests) since these variables are ubiquitously available across care settings, standardized, and low-burden to obtain, which makes models straightforward to deploy in real-world workflows \citep{read2023cancers,huang2022jco}.
We therefore treat structured EHRs as a deployment-friendly starting point; integrating imaging and notes is an important direction for future work once the structured-only baseline is firmly established.
}

% read2023cancers - An additional limitation is that these models do not incorporate additional clinical history such as the findings of prior EGD or colonoscopy procedures, but we intentionally chose to focus on readily ascertainable parameters from existing EHR data for easier potential use in the future.
% huang2022jco - There is increasing interest in leveraging EHR-based data sources for the development and validation of cancer risk models, as such models may be rapidly translated to real-world clinical practice.

Accordingly, we aimed to investigate a cross-cancer transfer-learning paradigm with deep learning based on EHR data: we pre-train a shared multilayer-perceptron (MLP) backbone on non-gastric GI/HPB cancers using routinely collected EHR variables (demographics, diagnoses/ICD codes, and laboratory values) and then adapt it to GC via fine-tuning. 
We evaluate this hypothesis retrospectively in the MIMIC-IV v3.1 EHR database \citep{Johnson2024MIMICIV}, restricting inputs to structured EHR data available without using diagnostic endoscopy or pathology.

{
\color{red}
From a clinical perspective, the proposed framework is especially
relevant for health systems that see relatively few GC cases but routinely manage
other gastrointestinal and hepatopancreatobiliary malignancies.
In such settings, our study shows how existing
structured EHR data and non-GC cancer cohorts can be reused to build a risk model
for GC under label scarcity without requiring new data collection.
Beyond GC, the same cross-cancer transfer framework can be directly adapted to
other underrepresented cancers or rare tumor subtypes where labeled data are
limited but related malignancies are more prevalent.
}

{
\color{red}
From a methodological standpoint, our framework is deliberately aligned with established multi-task and transfer-learning formulations~\citep{Caruana1997_MTL,Pan2010_TLsurvey,Yosinski2014_transferable}.
Classical multi-task learning typically trains a single model jointly on multiple tasks and reports performance averaged across those tasks.
In contrast, we treat colorectal, esophageal, liver, and pancreatic cancers as auxiliary source tasks that are used only during pretraining, and we evaluate performance exclusively on a single target task (gastric cancer) under varying degrees of label scarcity.
Our primary goal is to ask an applied question: to what extent can cross-cancer supervision on routine laboratory and comorbidity profiles improve GC risk modeling under label scarcity when using only structured EHR variables?
Accordingly, the contribution of this work is mainly empirical and translational: we provide the assessment of cross-cancer transfer on structured EHRs for GC, including (i) label-fraction sweeps that emulate low-resource GC settings, (ii) ablations on pretraining source-cancer composition, and (iii) comparisons between freezing versus full fine-tuning of the shared encoder.
}



%{
%%\color{red}
%Our objective is not to introduce a new algorithmic transfer-learning architecture,
%but to address a practical gap in the current EHR literature: although
%multi-task learning and transfer learning are well established, their
%application to structured EHR–based cancer risk modeling under real-world
%label scarcity has received limited empirical examination.
%Gastric cancer is a high-impact example where labels are sparse but related
%GI/HPB cancers are common.
%Thus, the novelty of this study lies in a translational contribution:
%providing a systematic, controlled, and clinically grounded evaluation of
%cross-cancer transferability within a realistic EHR setting, rather than
%proposing a fundamentally new algorithm.
%}


%Our contributions are threefold:
%(i) a multi-head transfer MLP with structural-parameter (L2-SP) regularization \citep{li2018l2sp} and controllable fine-tuning heads (including logit averaging) to stabilize adaptation; and
%(ii) a rigorous, seed-repeated selection protocol that fixes model choice by validation AUROC and then reports both validation and held-out test metrics with sensitivity/specificity/F1 at a validation-optimized threshold \citep{davis2006relationship,saito2015precision}.

%To test this hypothesis, we developed a reproducible pipeline on the MIMIC-IV v3.1 database to construct cohorts, engineer biologically meaningful features, and train several baseline models along with a transfer-learning multilayer perceptron (Transfer-MLP). The Transfer-MLP is pretrained on non-GC GI cancers (colorectal, liver, esophageal, pancreatic) and then fine-tuned on GC labels using a controlled head-selection strategy and structural parameter (SP) regularization. Our contributions are: (i) a transparent, fully code-driven cohort construction and feature engineering workflow that avoids features merely encoding physicians' suspicion; (ii) a multi-head transfer architecture that supports different fine-tuning strategies; and (iii) an empirical demonstration that pretraining on related cancers improves early GC prediction compared with GC-only training.


\section{Materials and Methods}

\subsection{Data Source and Ethics}
We used the publicly available de-identified MIMIC-IV clinical database (v3.1) \citep{Johnson2024MIMICIV}.
Access to MIMIC-IV was granted to all authors who completed the required CITI training,
and we adhered to all PhysioNet credentialing and data use agreements, including the prohibition of data disclosure and misuse.

\subsection{Cohort Construction and Features}
In this study, we constructed cohorts not only for gastric cancer (GC) but also for multiple non-GC cancers (colorectal, esophageal, liver, and pancreatic).
This design emphasizes that our approach leverages diverse cancer types for pre-training before fine-tuning on GC.
Throughout, we use the notations to denote cancer types: \textbf{GC} = gastric cancer (target task), \textbf{CC} = colorectal cancer, \textbf{EC} = esophageal cancer, \textbf{LC} = liver cancer, and \textbf{PC} = pancreatic cancer.

% and sampled controls at a fixed $1{:}3$ case:control ratio, excluding subjects with the target cancer.

For each cancer, we identified cases by the earliest qualifying ICD-9/10 diagnosis.
We constructed features as follows.
For cases, we anchored on the admission during which the cancer was diagnosed.
Features include demographics (age, sex), binary ICD-derived comorbidity indicators (e.g., diabetes, GERD, ulcers) from the history up to and including the admission start date, and the oldest available values of routine laboratory tests (CBC/CMP) observed within the 730 days prior to (and including) the admission start date (or from all available records for controls).

We restricted the inputs to routinely collected, structured EHR variables. 
Feature selection was informed by prior gastric cancer–related studies~\citep{park2024SHapley,huang2022jco,Kim2024EHRGC} 
and guided by the principle of prioritizing routinely measured, low-burden variables with minimal missingness 
(e.g., complete blood count [CBC] and comprehensive metabolic panel [CMP] components). 
Demographic variables included age and sex. 
ICD-derived binary comorbidity indicators encompassed diabetes mellitus; obesity; alcohol use disorder; current or prior tobacco use; gastroesophageal reflux disease (GERD); dyspepsia; peptic ulcer disease; chronic gastritis; \textit{Helicobacter pylori} infection; hypertension; history of myocardial infarction or angina; dyslipidemia or pure hypercholesterolemia; family history of cancer; protein–calorie malnutrition; post-hemorrhagic anemia; adverse effects of antineoplastic agents; dysphagia; receipt of antineoplastic chemotherapy; atrial fibrillation; iron-deficiency anemia; chronic kidney failure; coronary atherosclerosis or ischemic heart disease without angina; and hypo-osmolality or hyponatremia. 
Laboratory features consisted of CBC indices and basic chemistries, including hemoglobin, hematocrit, red blood cell count, white blood cell count, platelet count, mean corpuscular volume (MCV), mean corpuscular hemoglobin (MCH), mean corpuscular hemoglobin concentration (MCHC), red cell distribution width (RDW), leukocyte differential percentages (neutrophils, lymphocytes, monocytes, eosinophils, and basophils), sodium, potassium, chloride, bicarbonate, blood urea nitrogen, creatinine, glucose, and anion gap.


We then enforced a lab-complete inclusion criterion: patients with any missing value in the required laboratory feature set were excluded prior to modeling.
For each cancer, controls were sampled at a fixed $1{:}3$ case:control ratio from patients without a diagnosis of the cancer (ever), and the same lab-complete criterion was applied to controls.
Identical preprocessing was applied to cases and controls; numerical features were standardized (z-scored) using statistics computed on the training split only.



%\subsection{Feature Engineering}
%We engineered features that are interpretable and clinically plausible. Demographics included age and sex. Binary ICD-derived comorbidity indicators were computed for clinically relevant conditions (e.g., diabetes, obesity, GERD, dyspepsia, gastric ulcer, alcohol/tobacco use, anemia, malnutrition, atrial fibrillation, renal failure, lipid disorders). For laboratory features, we included the most recent value per analyte within a symmetric look-back window around the index date (or all available values for controls), covering complete blood count indices and common chemistries (e.g., hemoglobin, hematocrit, MCV/MCH/MCHC, WBC and differentials, platelets, RDW; sodium, potassium, chloride, bicarbonate, urea nitrogen, creatinine, glucose, anion gap). Missing values were imputed and missingness flags optionally dropped after imputation. Non-predictive index-related variables were excluded from modeling.


\subsection{Models}
Our primary objective is to quantify how much transfer learning from non-gastric cancers improves early prediction of gastric cancer (GC). 
We first introduce the transfer learning framework, which pre-trains models on multiple non-GC cancers and subsequently fine-tunes them on GC.
We then describe the non-transfer baselines trained solely on GC data for direct comparison.
To isolate the effect of transfer, we enforce identical feature sets, preprocessing steps, and stratified data splits across all methods.

\subsubsection{Transfer MLP}
Let $\mathcal{C}$ denote the set of non-gastric cancers used for pretraining; in our main configuration, $\mathcal{C}=\{\CC,\EC,\LC,\PC\}$ (i.e., colorectal, esophageal, liver, and pancreatic cancers, respectively).
For each $c\in\mathcal{C}$, let $\mathcal{D}_c=\{(x_i^{(c)},y_i^{(c)})\}_{i=1}^{N_c}$ be the corresponding case--control dataset (identical feature space across $c$), constructed with a case--control ratio of $1{:}3$ (one case per three controls).
Here $y\in\{0,1\}$ indicates the disease status for cancer type $c$ ($y{=}1$ for case with a confirmed diagnosis of type $c$, and $y{=}0$ for a control without that diagnosis).

Our deep learning model consists of a shared backbone encoder $f_\phi:\mathbb{R}^{d_{\mathrm{in}}}\!\to\!\mathbb{R}^{d_h}$ with model parameters $\phi$ and cancer-specific linear heads $\{h_{\theta_c}\}_{c\in\mathcal{C}}$ acting on the latent representation $u=f_\phi(x)$ for input features $x$.
Each head with model parameters $\theta_c=(w_c,b_c)\in\mathbb{R}^{d_h}\times\mathbb{R}$ is an affine map parameterized as
\[
h_{\theta_c}(u)=\langle w_c,u\rangle + b_c,
\]
and produces a logit $z_c(x)=h_{\theta_c}(f_\phi(x))$.
The backbone encoder $f_\phi$ is a feed-forward network with linear blocks, batch normalization \citep{Ioffe2015_BN}, ReLU, and dropout~\citep{Srivastava2014_Dropout}.

We first define the class-weighted binary cross-entropy (BCE) used throughout.
Let $\sigma(t)=\frac{1}{1+\exp(-t)}$ denote the logistic sigmoid.
Given a logit $z\in\mathbb{R}$ and a binary label $y\in\{0,1\}$, the per-sample loss is
\begin{equation}
  \label{eq:bce}
  \ell_{\mathrm{BCE}}(z,y;\alpha)
  = -\,\alpha\,y\log \sigma(z)\;-\;(1-y)\log\!\big(1-\sigma(z)\big),
  \end{equation}
where $\alpha>0$ is the positive-class weight; in our experiments we set $\alpha=3$ to match the $1{:}3$ case--control sampling design.
%TODO: cite: class-weighted BCE?

During pretraining, we jointly optimize the backbone $f_\phi$ and all cancer-specific heads $\Theta_h\!=\!\{\theta_c\}_{c\in\mathcal{C}}$ by minimizing the mean of the weighted BCE over cancer types:
\begin{equation}
  \frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}}
  \ \mathbb{E}_{(x,y)\sim\mathcal{D}_c}\!\big[\,\ell_{\mathrm{BCE}}(h_{\theta_c}(f_\phi(x)),y;\alpha)\,\big],
\end{equation}
with positive-class weight $\alpha{=}3$. 
To reduce overfitting, we applied standard $\ell_2$-regularization
(weight decay) with coefficient $\preDecayC$.
Thus, the pretraining objective is
\begin{equation}
  \label{eq:pre_obj}
%  \min_{\phi,\Theta_h}\ 
  \mathcal{L}_{\mathrm{pre}}(\phi,\Theta_h)
  = \frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}}
  \mathbb{E}_{(x,y)\sim\mathcal{D}_c}\!\big[\,\ell_{\mathrm{BCE}}(h_{\theta_c}(f_\phi(x)),y;\alpha)\,\big]
  + \preDecayC \big( \|\phi\|_2^2 + \sum_{c\in\mathcal{C}} \|\theta_c\|_2^2 \big).
\end{equation}

In practice, model training does not update the parameters using the entire dataset at once, but rather processes smaller subsets of data called mini-batches. 
A mini-batch is simply a small group of samples (e.g., a few dozen patients) drawn from the full dataset, and the model parameters are updated based on the gradient of the loss function computed by using only that group. 
This strategy reduces memory requirements and stabilizes optimization compared to using a single sample (stochastic updates) or the full dataset (full-batch updates).
During pretraining, at each optimization step we draw one mini-batch $B_c$ from each cancer type $c\in\mathcal{C}$ and compute their respective losses. 
The pretraining loss for a step is then obtained by averaging over all cancer types:
\begin{equation}
  \label{eq:pre_mix}
  \hat{\mathcal{L}}_{\mathrm{pre}}(\phi,\Theta_h)
  =\frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}}\ \frac{1}{|B_c|}\!
  \sum_{(x,y)\in B_c}\ell_{\mathrm{BCE}}(h_{\theta_c}(f_\phi(x)),y;\alpha) 
  + \preDecayC \Big( \|\phi\|_2^2 + \sum_{c\in\mathcal{C}} \|\theta_c\|_2^2 \Big).
\end{equation}
In this way, each cancer-specific head $h_{\theta_c}$ is trained using its own mini-batch, while the backbone encoder $f_\phi$ receives gradients aggregated across cancers, encouraging it to learn shared representations.

\textit{Expected effect.} Type-balanced, multi-task pretraining (\cref{eq:pre_obj,eq:pre_mix}) is expected to (i) induce cancer-agnostic risk representations in $f_\phi$, (ii) improve sample efficiency and stabilize optimization when fine-tuning on gastric cancer with limited labels, and (iii) mitigate label-imbalance effects through the weighted loss in \cref{eq:bce}.
These mechanisms are consistent with established benefits of multi-task learning and transfer learning~\citep{Caruana1997_MTL,Pan2010_TLsurvey,Yosinski2014_transferable,He2009_Imbalanced}.

For training the prediction model for GC, we introduce new GC linear head $\head{\GC}:\mathbb{R}^{d_{h}}\!\to\!\mathbb{R}$ whose model parameters $\headP{\GC}$ are randomly initialized.
For input features $x$, by using the pretrained backbone encoder $\enc{\encP}$, the probability of GC is predicted by applying the sigmoid function $\sigma$ to the logit $\head{\GC}(\enc{\encP}(x))$ (i.e., $\sigma(\head{\GC}(\enc{\encP}(x)))$).
Let $\pretrainP$ denote the pretrained model parameters of backbone encoder.
For fine-tuning, we first initialize the model parameters of backbone $\encP$ to $\pretrainP$, and then jointly optimize the backbone $\enc{\encP}$ and the GC head $\head{\GC}$ by minimizing the weighted BCE for the GC dataset:
\begin{equation}
\label{eq:ft_obj}
\mathcal{L}_{\mathrm{ft}}(\phi,\headP{\GC}) = \mathbb{E}_{(x,y)\sim \mathcal{D}_{\GC}}\!\big[\ell_{\mathrm{BCE}}(\head{\GC}(\enc{\pretrainP}(x)),y;\alpha)\big]\;+\;\ftDecayC \big( \|\phi\|_2^2 + \|\headP{\GC}\|_2^2 \big),
\end{equation}
where we use positive-class weight $\alpha{=}3$ and $\ftDecayC$ is the $\ell_2$-regularization coefficient for fine-tuning.
Details of other hyperparameters are provided in Appendix~\ref{app:hparam}.


% TODO: cite Adam
%For pretraining and fine-tuning phases, we optimize model parameters using the Adam optimizer with separate learning rates and $\ell_2$-regularization coefficients $\preDecayC$ and $\ftDecayC$.
%The mini-batch size is set to $\lfloor 0.2 \cdot N_c \rfloor$ capped to $[8,64]$ where $N_c$ is the data size for cancer type $c \in \{\CC,\EC,\LC,\PC,\GC\}$.
%All tasks share the positive-class weight $\alpha = 3$ in $\ell_{\mathrm{BCE}}$.


\subsubsection{Baselines (Non-transfer)}
We benchmark three widely used non-transfer families that reflect complementary inductive biases for structured EHR data \citep{EHRBreastRecurrence2023,WANG2023Prostate,park2024SHapley}. To avoid conflating modeling choices with pretraining, all baselines use the identical feature set, preprocessing, and stratified data splits as the Transfer MLP.

\subparagraph{Logistic Regression (LR)}
A linear probabilistic classifier that models the log-odds of GC as an affine function of the inputs, offering a strong, interpretable tabular baseline. LR captures additive effects and provides well-calibrated risk estimates under mild assumptions, making it a common comparator in clinical prediction studies.

\subparagraph{eXtreme Gradient Boosting (XGB)}
An ensemble of decision trees trained by gradient boosting to approximate non-linear decision boundaries and higher-order feature interactions on tabular data. This class of models is competitive on EHR tasks due to its flexibility with mixed-scale features and robustness to outliers and sparse signals.

\subparagraph{MLP trained from scratch (MLP)}
To isolate the effect of transfer learning, we also evaluate a multilayer perceptron that is architecturally identical to the Transfer MLP’s backbone and GC head (linear blocks with batch normalization, ReLU, and dropout), and uses the same training objective, mini-batch formulation, and regularization as in fine-tuning. The only difference is that the pretraining stage on non-GC cancer datasets is omitted: all parameters are randomly initialized and optimized solely on the GC dataset. Conceptually, this baseline serves as an ablation of the Transfer MLP in which the pretraining component is removed; thus, any performance gap directly quantifies the benefit of transfer.


\subsection{Statistical Analysis}
%In imbalanced settings, AP is often more informative than AUROC~\citep{Saito2015_PR}.
For each cancer type, we first divided the subjects into training and test sets, using an 85\%--15\% split. 
We further split the training set so that 20\% of it was used as a validation set. 
In all splits, stratified sampling was performed to ensure that the proportion of cases and controls remained consistent across the training, validation, and test sets. 
For a given hyperparameter setting, we trained model parameters using the training set, and computed the model performances for the validation and test sets. 
The evaluation metrics included the area under the receiver operating characteristic curve (AUROC), average precision (AP), sensitivity, specificity, and F1-score. 
This entire procedure was repeated 10 times with different data splits. 
To optimize hyperparameters, we computed the mean AUROC on the validation set across these repetitions and selected hyperparameters based on this mean AUROC. 
The final model performance was then evaluated on the independent test split, and we report the mean performance across the 10 repetitions as the final test result. 
\textcolor{red}{
To quantify both model stability and uncertainty, we also report standard deviations or 95\% confidence intervals across repetitions. 
In addition, we computed 95\% confidence intervals (CIs) for AUROC using patient-level bootstrap resampling of the test set with 2000 resamples.
To formally assess whether differences in AUROC between models sharing the same test set were statistically significant, 
we also applied DeLong's test for ROC curves to obtain $p$-values for 
pairwise AUROC comparisons between the Transfer model and each baseline (LR, XGB, and scratch MLP).
}

Metrics such as AUROC and average precision (AP) were computed directly from the continuous prediction scores without applying any threshold. 
For threshold-dependent measures, operating points were determined using the validation split and then applied to the independent test split. 
Specifically, the threshold for the F1-score was chosen as the point that maximized the validation F1, and this same threshold was used to compute the F1-score on the test set. 
For sensitivity and specificity, following the approach in \citep{read2023cancers}, we selected the threshold that provided the best balance between the two metrics—that is, the point on the validation receiver operating characteristic (ROC) curve that was closest to the ideal point (FPR=0, TPR=1) on the validation ROC curve.
All reported sensitivity and specificity values on the test set were based on this validation-derived threshold.

{
\color{red}
To provide explanations of the Transfer model, we additionally computed SHapley Additive Explanation (SHAP) values.
SHAP attributions for the neural network were obtained using the DeepLIFT-based approximation for deep models~\citep{Lundberg2017SHAP,Shrikumar2017DeepLIFT}.
For each patient in the held-out GC test set, SHAP values were calculated for every input feature, and feature importance was summarized as the mean absolute SHAP value across patients.
The larger values indicate a greater overall influence of that feature on the model's predicted GC risk.
}


%\subsection{Hyperparameter Search}
%\label{subsec:hparam}
%We optimized model hyperparameters by a one–factor–at–a–time (OFAT) sweep for the \emph{Transfer} model.
%The selection criterion was the mean validation AUROC across 10 repetitions.
%The neural model is optimized by using the Adam optimizer \cite{kingma2015adam}.
%The mini-batch size is set to $\lfloor 0.05 \cdot N_c \rfloor$ capped to $[8,64]$ where $N_c$ is the data size for cancer type $c \in \{\CC,\EC,\LC,\PC,\GC\}$.
%Starting from a central configuration in Table~\ref{tab:hparam-transfer-ofat}, we perform a one–factor–at–a–time (OFAT) sweep by varying \emph{one} hyperparameter at a time over the ranges in Table~\ref{tab:hparam-transfer-ofat} while keeping others fixed.
%
%\begin{table}[htbp]
%\caption{Transfer–MLP OFAT hyperparameter ranges.\label{tab:hparam-transfer-ofat}}
%\centering
%\begin{tabularx}{\textwidth}{CC}
%\toprule
%Hyperparameter & Values \\
%\midrule
%Architecture (layers) & \([128,64],\ [64,32],\ [32,16]\) \\
%Dropout & \(0.2\) (fixed) \\
%Pretraining LR & \(10^{-2},\ 10^{-3},\ 10^{-4}\) \\
%Pretraining weight decay & \(10^{-2},\ 10^{-3},\ 10^{-4}\) \\
%Pretraining steps & $1000, 2000, 5000$\\
%Fine–tuning LR & \(10^{-3},\ 10^{-4},\ 10^{-5}\) \\
%Fine–tuning weight decay & \(10^{-3},\ 10^{-4},\ 10^{-5}\) \\
%Fine–tuning epochs & \(20,\ 50,\ 100\) \\
%\bottomrule
%\end{tabularx}
%\end{table}


\section{Results}
\subsection{Cohort Overview}
\begin{table}[htbp]
\caption{Cohort summary for the target (gastric cancer; GC) and source cancers (colorectal, esophageal, liver, pancreatic). Sex is male proportion (\%), and age is reported as median [IQR].\label{tab:cohort}}
\centering
\begin{tabularx}{\textwidth}{CCCCP{4cm}}
\toprule
Cohort & Cases & Controls & Male (\%) & Age (Median [IQR]) \\
\midrule
GC & 508 & 1524 & 51.6 & 58.0 [41.0–71.0] \\
CC & 1296 & 3888 & 47.1 & 57.0 [39.0–71.2] \\
EC & 364 & 1092 & 52.9 & 58.0 [40.0–71.0] \\
LC & 1432 & 4296 & 53.1 & 58.0 [40.0–70.0] \\
PC & 1467 & 4401 & 48.2 & 60.0 [42.0–72.0] \\
%GC & 731 & 2193 & 51.1 & 55.0 [35.0–69.0] \\
%CC & 2020 & 6060 & 49.5 & 55.0 [35.0–70.0] \\
%EC & 529 & 1587 & 55.1 & 56.0 [36.0–69.0] \\
%LC & 1771 & 5313 & 53.3 & 55.0 [34.0–68.0] \\
%PC & 1958 & 5874 & 48.6 & 56.0 [34.0–70.0] \\
\bottomrule
\end{tabularx}
\end{table}
We constructed case--control cohorts for GC (target) and four source cancers (CC, EC, LC, PC) at a 1:3 ratio.
Table~\ref{tab:cohort} summarizes sample sizes and demographics.
Across cohorts, the sex distribution was approximately balanced overall (weighted mean $\approx$50\% male), and age medians clustered around the late 50s (57--60 years) with interquartile ranges spanning roughly the early 40s to the early 70s.
Within the pretraining pool, EC constitutes the smallest cohort (364 cases, 1{,}092 controls), whereas PC provides the largest case count (1{,}467), followed by LC (1{,}432) and CC (1{,}296).
In aggregate, the non-GC sources contribute 4{,}559 cases and 13{,}677 controls (total 18{,}236), compared with 508 and 1{,}524 for GC (total 2{,}032); thus, the pretraining pool supplies approximately nine times more labeled samples than the GC task alone, a scale difference pertinent for transfer learning before adaptation to GC.


\subsection{Comparison of Models}
%\input{tex_and_figs/table_model_compare_gc1.tex}
\begin{table}[htbp]
\caption{Performance of models.\label{tab:model-compare-gc1} {\color{red} Reported values include the mean test performance across repeated runs with standard deviation ($\pm$), and the 95\% bootstrap confidence interval (parentheses).}}
%\begin{adjustwidth}{-\extralength}{0cm}
\small
\begin{tabularx}{\textwidth}{P{1cm}P{4cm}CCCCC}
\toprule
Model & AUROC & AP & Sensitivity & Specificity & F1 & Brier \\
\midrule
%LR & 0.812 & 0.518 & 0.707 & 0.755 & 0.578 \\
%XGB & 0.808 & 0.549 & 0.692 & 0.737 & 0.556 \\
%MLP & 0.830 & 0.603 & 0.782 & 0.738 & 0.609 \\
%Transfer & 0.854 & 0.600 & 0.768 & 0.786 & 0.636 \\
LR & $0.812${\color{red}$\pm0.002$ (0.759--0.862)} & 0.518 & 0.707 & 0.755 & 0.578 & 0.151 \\
XGB & $0.808${\color{red}$\pm0.010$ (0.760--0.851)} & 0.549 & 0.692 & 0.737 & 0.556 & 0.160 \\
MLP & $0.830${\color{red}$\pm0.006$ (0.780--0.874)} & 0.603 & 0.782 & 0.739 & 0.609 & 0.164 \\
Transfer & $0.854${\color{red}$\pm0.007$ (0.811--0.893)} & 0.600 & 0.768 & 0.786 & 0.636 & 0.159 \\
\bottomrule
\end{tabularx}
%\end{adjustwidth}
\end{table}
As summarized in Table~\ref{tab:model-compare-gc1}, the Transfer model achieved the highest discrimination and the strongest overall operating profile on the full-label setting.
Transfer obtained the top AUROC ($0.854$) and the highest F1 ($0.636$), with the best specificity ($0.786$).
Average precision (AP) was comparable to the scratch MLP (Transfer $0.600$ vs.\ MLP $0.603$) and higher than LR ($0.518$) and XGB ($0.549$).
Sensitivity for the MLP ($0.782$) slightly exceeded that of Transfer ($0.768$).
Overall, relative to the scratch MLP, Transfer improved AUROC by $+0.024$ and F1 by $+0.027$ (and specificity by $+0.048$), at the cost of a marginally lower AP ($-0.003$), indicating that pretraining on related non-GC cancers yields more informative representations for GC risk despite a small AP trade-off.

%\textcolor{red}{
%Across the ten repetitions, the standard deviations of test AUROC were small for all models (e.g., ${\leq}0.01$), suggesting stable performance estimates rather than isolated favorable runs.
%For AUROC, the 95\% bootstrap confidence interval for the Transfer model (0.816--0.896) was shifted upward relative to that of the scratch MLP (0.781--0.883), consistent with a modest but reproducible gain.
%%In pairwise AUROC comparisons on the common test set, DeLong’s test indicated that the improvement of Transfer over MLP was [statistically significant at the $\alpha{=}0.05$ level / numerically higher but did not reach conventional significance] (Table~\ref{tab:delong-main}), while improvements over LR and XGB were [also significant / of similar magnitude].
%Taken together, these results support that the observed AUROC differences are modest in absolute terms but stable across repetitions.
%%Thus, the AUROC differences between Transfer and MLP should be interpreted as small and numerically consistent rather than as a large or definitively significant effect.
%}

%\textcolor{red}{
%Across the ten repetitions, the standard deviations of test AUROC were small for all models ($\leq 0.01$), suggesting stable performance estimates rather than isolated favorable runs.
%For AUROC, the 95\% bootstrap confidence interval for the Transfer model (0.816--0.896) was shifted upward relative to that of the scratch MLP (0.781--0.883), consistent with a modest but reproducible gain.
%Although the CIs partially overlap, the means and narrow standard deviations across repetitions together indicate that the observed AUROC advantage of the Transfer model over the baselines is modest in absolute magnitude but statistically consistent and not driven by a single random training.
%}

\textcolor{red}{
Across the ten repetitions, the standard deviations of test AUROC were small for all models ($\leq 0.01$), suggesting stable performance estimates rather than isolated favorable runs.
For AUROC, the 95\% bootstrap confidence interval for the Transfer model (0.816--0.896) was shifted upward relative to that of the scratch MLP (0.781--0.883), consistent with a modest but reproducible gain.
Because the held-out GC test set contains a limited number of positive cases, these bootstrap CIs are relatively wide and partially overlapping across models.
In this sense, the width of the CIs primarily quantifies how much the performance would fluctuate if a similarly sized test cohort were resampled, while the consistent shift in the mean AUROC toward the Transfer model suggests a small but robust advantage.
}

\textcolor{red}{
Pairwise AUROC comparisons using DeLong's test mirrored this pattern.
On the common GC test set, the Transfer model achieved significantly higher AUROC than LR and XGB 
($p=0.0023$ and $p=0.0433$, respectively), whereas its advantage over the scratch MLP did not reach 
conventional statistical significance ($p=0.1231$).
Given the moderate size of the held-out GC test set and the limited number of positive cases, 
this non-significant result for the Transfer versus MLP comparison likely reflects both the modest effect size 
and the finite-sample uncertainty rather than the absence of any performance difference.
}



\subsection{Model performance under reduced GC training labels}
%\begin{table}[t]
%\centering
%\small
%\begin{tabular}{lllllll}
%\toprule
%gc\textbackslash \_rate & Model & AUROC & AP & Sensitivity & Specificity & F1 \\
%\midrule
%0.10 & LR & 0.787 & 0.491 & 0.718 & 0.714 & 0.557 \\
%0.10 & XGB & 0.698 & 0.430 & 0.637 & 0.633 & 0.465 \\
%0.10 & MLP & 0.745 & 0.453 & 0.653 & 0.709 & 0.516 \\
%0.10 & Transfer & 0.827 & 0.560 & 0.722 & 0.759 & 0.589 \\
%0.20 & LR & 0.809 & 0.506 & 0.762 & 0.725 & 0.588 \\
%0.20 & XGB & 0.737 & 0.467 & 0.622 & 0.694 & 0.490 \\
%0.20 & MLP & 0.780 & 0.478 & 0.672 & 0.744 & 0.549 \\
%0.20 & Transfer & 0.832 & 0.572 & 0.737 & 0.764 & 0.602 \\
%0.50 & LR & 0.809 & 0.514 & 0.734 & 0.745 & 0.586 \\
%0.50 & XGB & 0.787 & 0.521 & 0.692 & 0.725 & 0.549 \\
%0.50 & MLP & 0.807 & 0.543 & 0.736 & 0.742 & 0.585 \\
%0.50 & Transfer & 0.849 & 0.582 & 0.767 & 0.774 & 0.627 \\
%1.00 & LR & 0.812 & 0.518 & 0.707 & 0.755 & 0.578 \\
%1.00 & XGB & 0.808 & 0.549 & 0.692 & 0.737 & 0.556 \\
%1.00 & MLP & 0.830 & 0.603 & 0.782 & 0.738 & 0.609 \\
%1.00 & Transfer & 0.854 & 0.600 & 0.768 & 0.786 & 0.636 \\
%\bottomrule
%\end{tabular}
%\caption{Performance across the GC sampling rate $r$. Lower $r$ simulates fewer labeled GC \emph{training} cases.\label{tab:model-gc-rate-sweep}}
%\end{table}
\begin{figure}[htbp]
\centering
\subfloat[AUROC]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_model_gc_rate_auroc.pdf}}
\subfloat[AP]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_model_gc_rate_ap.pdf}} \\
\subfloat[F1]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_model_gc_rate_f1.pdf}}
\caption{Performance across the GC sampling rate $r$. Lower $r$ simulates fewer labeled GC training cases. \textcolor{red}{Error bars denote 95\% confidence intervals across repetitions.}\label{fig:model-gcrate}}
\end{figure}
As summarized in Figure~\ref{fig:model-gcrate} (AUROC, AP, and F1 panels), we varied the GC sampling rate $r \in \{1.0, 0.5, 0.2, 0.1\}$ to quantify robustness under limited GC labels.
For each $r$, we applied stratified sampling at proportion $r$ on the GC training partition (preserving the case{:}control ratio), while keeping the GC validation and test partitions fixed.
Non-GC source datasets for pretraining and all preprocessing were unchanged across $r$.
All models (LR, XGB, MLP-from-scratch, and Transfer) were then trained on the resulting $r$-subsampled GC training set for that rate.
The Transfer model always used the same pretraining configuration on non-GC cancers and was fine-tuned on the $r$-subsampled GC training set.

As $r$ decreases, all models show the expected degradation in AUROC, AP, and F1 due to reduced sample size (see Figure~\ref{fig:model-gcrate}).
Across all rates, the Transfer model maintains the best discrimination and precision--recall performance, with the absolute gains most pronounced at the smallest $r$.
These findings indicate that pretraining on related non-GC cancers consistently improves GC risk modeling across a wide range of labeled-data regimes, with the largest improvements in AP and F1 under the scarcest-label settings.
%At \texttt{gc\_rate}$=0.01$, Transfer achieves AUROC~0.916 and AP~0.765, surpassing the scratch MLP (AUROC~0.813, AP~0.546) by $+0.103$ AUROC and $+0.219$ AP, and yielding the highest F1 (0.715 vs.\ 0.600; $+0.115$).
%The advantage remains substantial at \texttt{gc\_rate}$=0.02$ (AUROC $+0.075$, AP $+0.165$, F1 $+0.088$) and \texttt{gc\_rate}$=0.05$ (AUROC $+0.058$, AP $+0.133$, F1 $+0.085$). 
\textcolor{red}{
When visualized with repetition-level confidence intervals in Figure~\ref{fig:model-gcrate}, this effect is also reflected in model stability.
As $r$ decreases, the error bars for LR, XGB, and the scratch MLP can widen, indicating increasing variability across repeated training runs.
In contrast, the Transfer model maintains relatively narrow and nearly constant intervals over $r$, suggesting that cross-cancer pretraining yields more stable performance estimates under label scarcity and reduces sensitivity to sampling variability in the GC training cohort.
}

%With more abundant labels, the gap narrows but persists.
%At \texttt{gc\_rate}$=0.10$, Transfer improves over MLP by $+0.040$ AUROC, $+0.097$ AP, and $+0.048$ F1; at \texttt{gc\_rate}$=0.50$, the corresponding margins are $+0.013$, $+0.027$, and $+0.027$; and at \texttt{gc\_rate}$=1.00$, Transfer still leads (AUROC 0.938 vs.\ 0.930; AP 0.826 vs.\ 0.811; F1 0.751 vs.\ 0.735).
%Notably, AUROC is relatively robust to prevalence, yet non-transfer models degrade markedly at \texttt{gc\_rate}$\le 0.05$, whereas Transfer retains higher AUROC (0.916--0.924) and substantially higher AP (0.765--0.787). 




\subsection{Effect of pretraining source composition}
%\input{tex_and_figs/table_transfer_by_pretrain_types.tex}
\begin{table}[htbp]
\caption{The performance of \emph{Transfer} grouped by pre-train cancer types. {\color{red} Reported values include the mean test performance across repeated runs with standard deviation ($\pm$).}\label{tab:transfer-by-pretrain-types}}
\small
\begin{tabularx}{\textwidth}{P{2.5cm}P{2.5cm}CCCCC}
\toprule
Pretrain types & AUROC & AP & Sensitivity & Specificity & F1 & Brier \\
\midrule
%LC & 0.931 & 0.809 & 0.875 & 0.839 & 0.743 \\
%EC & 0.932 & 0.811 & 0.878 & 0.841 & 0.747 \\
%CC & 0.932 & 0.815 & 0.866 & 0.849 & 0.748 \\
%PC & 0.934 & 0.816 & 0.873 & 0.848 & 0.750 \\
%CC+EC+LC+PC & 0.938 & 0.826 & 0.890 & 0.838 & 0.751 \\
%CC & 0.841 & 0.568 & 0.792 & 0.763 & 0.632 \\
%PC & 0.838 & 0.572 & 0.758 & 0.743 & 0.599 \\
%LC & 0.835 & 0.544 & 0.767 & 0.745 & 0.605 \\
%EC & 0.828 & 0.578 & 0.705 & 0.779 & 0.594 \\
%CC+EC+LC+PC & 0.854 & 0.600 & 0.768 & 0.786 & 0.636 \\
%\midrule
CC & $0.841${\color{red}$\pm0.006$} & 0.568 & 0.792 & 0.763 & 0.632 & 0.171 \\
PC & $0.838${\color{red}$\pm0.005$} & 0.572 & 0.758 & 0.743 & 0.599 & 0.169 \\
LC & $0.835${\color{red}$\pm0.007$} & 0.544 & 0.767 & 0.745 & 0.605 & 0.173 \\
EC & $0.828${\color{red}$\pm0.005$} & 0.578 & 0.705 & 0.779 & 0.594 & 0.168 \\
CC+EC+LC+PC & $0.854${\color{red}$\pm0.007$} & 0.600 & 0.768 & 0.786 & 0.636 & 0.159 \\
\bottomrule
\end{tabularx}
\end{table}
To examine how pretraining source composition influences transfer performance,
we repeated pretraining using one source cancer at a time and compared the results
to the model pretrained on all four non-GC cancers (CC, EC, LC, PC).
As shown in Table~\ref{tab:transfer-by-pretrain-types}, single–source transfer models achieved
AUROC values in the 0.828–0.841 range and F1 in the 0.594–0.632 range, with AP between 0.544 and 0.578.
Among the single sources, CC yielded the highest F1 (0.632), while EC gave the highest AP (0.578).
The multi–source model using CC+EC+LC+PC achieved the best overall operating profile
(AUROC 0.854, F1 0.636) and an AP of 0.600.
These results suggest that combining heterogeneous sources offers a modest but consistent benefit in discrimination and overall operating balance.
While the multi-source model performed best in this experiment, 
we do not claim that this combination is universally optimal. 
Instead, these findings suggest that leveraging multiple heterogeneous source cancers 
can provide complementary EHR-visible signals, 
and that carefully balancing or selecting diverse sources 
offers potential to further improve generalization for gastric cancer prediction.



%\subsection{Primary Test Performance}
%We compare LR, XGB, and single-task MLP trained on GC only with the proposed Transfer-MLP (pretrained on CC/EC/LC/PC, fine-tuned on GC).
%Performance is reported as AUROC and AP on the held-out test set;
%Sensitivity, Specificity, and F1 are computed at the validation-selected threshold $t^\star$ that maximizes F1. % (Code refs: Train_MLP/LR/XGB, get_best_f1_threshold)
%% :contentReference[oaicite:32]{index=32} :contentReference[oaicite:33]{index=33} :contentReference[oaicite:34]{index=34}
%We show mean $\pm$ SD over five seeds and 95\% CIs from patient-level bootstrap.
%Table~\ref{tab:main} presents the results.


\subsection{Freezing versus full fine-tuning under limited GC labels}
%\begin{table}[t]
%	\centering
%	\small
%	\begin{tabular}{llrrrrr}
%	\toprule
%	gc\textbackslash \_rate & Variant & AUROC & AP & Sensitivity & Specificity & F1 \\
%	\midrule
%0.10 & FT-Head & 0.825000 & 0.545000 & 0.729000 & 0.750000 & 0.586000 \\
%0.10 & FT-Full & 0.827000 & 0.560000 & 0.722000 & 0.759000 & 0.589000 \\
%0.20 & FT-Head & 0.827000 & 0.553000 & 0.766000 & 0.741000 & 0.602000 \\
%0.20 & FT-Full & 0.832000 & 0.572000 & 0.737000 & 0.764000 & 0.602000 \\
%0.50 & FT-Head & 0.831000 & 0.565000 & 0.754000 & 0.753000 & 0.603000 \\
%0.50 & FT-Full & 0.849000 & 0.582000 & 0.767000 & 0.774000 & 0.627000 \\
%1.00 & FT-Head & 0.834000 & 0.566000 & 0.755000 & 0.756000 & 0.607000 \\
%1.00 & FT-Full & 0.854000 & 0.600000 & 0.768000 & 0.786000 & 0.636000 \\
%	\bottomrule
%	\end{tabular}
%	\caption{Effect of freezing vs.\ full fine-tuning across GC training label fractions $r$.\label{tab:transfer-combo-gc-rate}}
%\end{table}
\begin{figure}[htbp]
	\centering
	\subfloat[AUROC]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_transfer_combo_gc_rate_auroc.pdf}}
	\subfloat[AP]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_transfer_combo_gc_rate_ap.pdf}}\\
	\subfloat[F1]{\includegraphics[width=0.48\textwidth]{tex_and_figs/fig_transfer_combo_gc_rate_f1.pdf}}
	\caption{Effect of freezing vs.\ full fine-tuning across GC training label fractions $r$. \textcolor{red}{Error bars denote 95\% confidence intervals across repetitions.}\label{fig:transfer-combo-gcrate}}
\end{figure}
We investigated whether a pretrained backbone should remain fixed during adaptation to gastric cancer (GC) or be allowed to update. Although our pretraining sources (CC/EC/LC/PC) are related to GC, their feature distributions and label–feature couplings are not identical; some degree of source-to-target shift is therefore expected. In such settings, transfer learning and multi-task learning can confer advantages by first inducing shared, cancer-agnostic representations and then adapting them to the target task \citep{Caruana1997_MTL,Pan2010_TLsurvey,Yosinski2014_transferable}. The practical question, however, is whether adaptation should occur only in a newly introduced GC head (with the backbone frozen; \emph{FT-Head}) or across all layers (with the backbone unfrozen; \emph{FT-Full}).

To answer this, we compared FT-Head and FT-Full across GC training-label rates $r \in \{1.0, 0.5, 0.2, 0.1\}$: \emph{FT-Head}, which freezes the pretrained backbone and trains only a new linear GC head, and \emph{FT-Full}, which jointly fine-tunes the backbone and the GC head. Both variants shared the same pretraining on CC+EC+LC+PC, identical features, preprocessing, and stratified splits. 
Figure~\ref{fig:transfer-combo-gcrate} summarizes discrimination and precision–recall performance.
Overall, full fine-tuning (FT-Full) outperformed freezing (FT-Head) at every label fraction, with larger gains as more GC supervision was available.

These results are consistent with two complementary mechanisms. First, allowing the backbone to update helps realign the shared representation to GC-specific covariances, improving ranking and precision–recall when the target signal is sufficiently informative \citep{Pan2010_TLsurvey,Yosinski2014_transferable}. Second, when labels are extremely scarce, freezing the backbone reduces the effective degrees of freedom, tempering variance and occasionally narrowing the gap on threshold-sensitive metrics, even if overall discrimination remains similar.

In light of the observed margins at moderate-to-higher label availability and for conceptual consistency, we use FT-Full as the primary transfer setting, using FT-Head as an ablation to quantify the value of updating the representation during adaptation.



\subsection{Feature importance patterns (SHAP)}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{tex_and_figs/Transfer_shap_summary_bar_global.pdf}
\caption{Mean absolute SHAP values (computed using a DeepLIFT-based SHAP approximation) for the Transfer model.
%Bars show the average absolute contribution of each feature to the model's predicted gastric cancer risk on the held-out test set; larger values indicate a greater overall influence on predictions.
}
\label{fig:shap-global}
\end{figure}
{
\color{red}
Global feature importance for the Transfer model is summarized in Figure~\ref{fig:shap-global} using mean absolute SHAP values computed on the held-out GC test set.
The top 15 features by mean absolute SHAP value comprised comorbid and exposure-related variables (hypertension, smoking status, family history of cancer, gastroesophageal reflux disease, protein--calorie malnutrition, coronary atherosclerosis, and adverse effects of antineoplastic agents), blood-based indices from the complete blood count (hemoglobin, white blood cell count, red cell distribution width, and monocyte percentage), metabolic/acid--base markers (anion gap and bicarbonate), and basic demographics (age and male sex).
These results indicate that the model relies on a mixture of demographic, comorbidity, and routine laboratory variables.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
In this retrospective study using de-identified structured EHR data from MIMIC-IV v3.1~\citep{Johnson2024MIMICIV}, a transfer learning strategy that pre-trains a multilayer perceptron on non-gastric cancers (CC/EC/LC/PC) and then adapts to GC achieved the best overall performance among strong non-transfer baselines. On the full GC label regime ($r{=}1.00$), Transfer attained the highest AUROC and F1 (Table~\ref{tab:model-compare-gc1}); AP was essentially tied with the scratch MLP (Transfer $0.600$ vs.\ MLP $0.603$).
When we progressively reduced GC training labels ($r\!\downarrow$), Transfer preserved a consistent AUROC advantage and typically higher AP—with the largest margins under the scarcest labels (Figure~\ref{fig:model-gcrate}; e.g., at $r{=}0.1$, Transfer vs.\ scratch MLP: AUROC $0.827$ vs.\ $0.745$, AP $0.560$ vs.\ $0.453$, F1 $0.589$ vs.\ $0.516$).
We also observed that full fine-tuning generally outperformed frozen-backbone adaptation once modest GC supervision was available, while freezing remained competitive at the most extreme data scarcity (Figure~\ref{fig:transfer-combo-gcrate}).
Finally, varying the composition of pretraining sources showed that combining four sources (CC+EC+LC+PC) produced the best overall performance—raising AUROC and F1 over the scratch MLP and yielding an AP essentially on par with it (Table~\ref{tab:transfer-by-pretrain-types}).

Our findings align with prior observations that risk signals visible in structured EHR data recur across GI/HPB malignancies and therefore can act as transferable supervision for GC~\citep{read2023cancers,Aksoy2019CBC,Krieg2024IDA,Kim2014GIBleed,Stein2016Anemia,Gkamprela2017CLDIDA,Crumley2010Albumin,Shimoyama2013DMGC,Yoon2013DMGC,Guo2022DMGC}. While earlier GC prediction efforts have largely optimized within-cancer models using routine variables~\citep{park2024SHapley,huang2022jco,Kim2024EHRGC}, our results suggest that cross-cancer pretraining helps distill cancer-agnostic representations from noisy labs and comorbidities, which then transfer to GC with improved sample efficiency. This is consistent with established benefits of multi-task and transfer learning on related tasks~\citep{Caruana1997_MTL,Pan2010_TLsurvey,Yosinski2014_transferable}.

\textcolor{red}{
Our findings are consistent with prior reports that transfer learning improves oncologic prediction under label scarcity across various data modalities.
In medical imaging, models pre-trained on large source datasets and then fine-tuned on smaller tumor-specific cohorts routinely yield higher discrimination and better generalization \citep{Kim2022TLReview}.
In omics, cross-cancer transfer learning improves survival or progression prediction \citep{LopezGarcia2020PanCancerTL,Hanczar2022TLGeneExp}.
%Recent multi-task formulations that learn shared representations across cancers likewise report the largest gains for smaller cohorts~\citep{Lin2024MTLGNN}.
Our cross-cancer transfer on structured EHR variables parallels these patterns and suggests that cancer-agnostic signals learned from routine labs and comorbidity profiles can be reused to stabilize GC modeling when labels are limited.
}


{\color{red}
We note that the absolute improvements in AUROC are modest when comparing Transfer with a scratch MLP (Table~\ref{tab:model-compare-gc1}).
As detailed in the Results section, these gains are consistently observed across repetitions and bootstrap-based uncertainty estimates, despite the moderate size of the held-out GC test set, but we do not claim a dramatic effect size.
Rather, the main value of the proposed framework lies in its stability and behavior under label scarcity.
As GC training labels are reduced, the relative advantage of Transfer becomes more pronounced in AUROC, AP, and F1 (Figure~\ref{fig:model-gcrate}), suggesting that pretraining on related cancers yields a more stable operating profile when data are limited.
From a clinical standpoint, such modest but consistent gains in discrimination and F1 at fixed endoscopy capacity can still translate into more accurate prioritization of patients for evaluation, especially in systems where GC labels are sparse but other GI/HPB cancers are commonly seen.
In line with this, DeLong's test showed statistically significant AUROC improvements of the Transfer model over LR and XGB, 
while the comparison versus the scratch MLP did not reach conventional significance ($p=0.1231$), 
which is compatible with a small effect size and limited statistical power given the moderate GC test cohort size.
}




We hypothesize three complementary mechanisms behind the observed gains:
(i) \textit{representation sharing}—pretraining encourages the backbone to encode recurring EHR patterns that GC also exhibits;
(ii) \textit{regularization under scarcity}—pretraining reduces the effective hypothesis space, aiding generalization when GC labels are limited;
and (iii) \textit{optimization stability}—a pretrained initialization improves convergence to better optima compared with training from scratch. The superiority of full fine-tuning over freezing at moderate data availability suggests that some GC-specific re-alignment of shared features is beneficial, whereas freezing can mitigate variance when labels are extremely scarce, producing similar discrimination with fewer trainable degrees of freedom. These observations match classic transfer learning trade-offs between bias and variance~\citep{Pan2010_TLsurvey,Yosinski2014_transferable}.
\textcolor{red}{
Consistent with this interpretation, the repetition-level confidence intervals in Figure~\ref{fig:model-gcrate} remain relatively tight and nearly invariant across GC sampling rates for the Transfer model, whereas most baselines exhibit visibly larger dispersion at lower rates.
This pattern supports the view that pretraining acts as an implicit regularizer that dampens sensitivity to sampling noise in small GC cohorts.
}

%All single-source pretraining choices (CC, EC, LC, PC) provided meaningful improvements over GC-only training, and combining all four delivered the strongest overall results (Table~\ref{tab:transfer-by-pretrain-types}).
Compared to single-source pretraining choices (CC, EC, LC, PC), combining all four pretraining choices delivered the strongest overall balance—improving AUROC and F1 versus scratch and yielding an AP essentially on par with it (Table~\ref{tab:transfer-by-pretrain-types}).
These results imply that (i) each source cancer contributes overlapping but not identical EHR-visible signals; and (ii) diversity across sources can add incremental value. We do not claim that our specific mix is universally optimal; rather, our results motivate future work on principled source selection and weighting schemes that account for relatedness, cohort size, and label quality.

{
\color{red}
To further contextualize the model behavior, we inspected global feature importance for the main Transfer model using mean absolute SHAP values (DeepLIFT-based) on the held-out GC test set (Figure~\ref{fig:shap-global}).
Rather than being driven by a single variable, the model primarily relies on several clinically meaningful axes: 
(i) anemia and nutritional status, reflected by hemoglobin, red cell distribution width, and protein--calorie malnutrition; 
(ii) systemic inflammation and immune activation, captured by white blood cell count and monocyte percentage; 
(iii) metabolic and acid--base derangements, represented by anion gap and bicarbonate; 
(iv) cardiometabolic and oncologic comorbidities and exposures, including hypertension, smoking, family history of cancer, coronary atherosclerosis, gastroesophageal reflux disease, and adverse effects of antineoplastic agents; and 
(v) baseline demographic risk (age and sex).
These groupings are consistent with established clinical links between iron-deficiency anemia, malnutrition, chronic inflammation, cardiometabolic burden, familial predisposition, and gastrointestinal malignancy, supporting the view that the Transfer model is relying on clinically plausible EHR signals rather than spurious artifacts.
}

%From a clinical standpoint, these models are not intended to replace diagnostic endoscopy but to prioritize patients for timely evaluation and follow-up. Earlier identification of higher-risk patients in routine care could shorten time-to-endoscopy and facilitate detection at more treatable stages, where survival is markedly better~\citep{NCI2023stomach}. The transfer framework is appealing for institutions with few confirmed GC cases but access to other GI/HPB cancer cohorts; pretraining on related cancers can “borrow strength” and stabilize GC performance without requiring specialized data modalities or free-text notes.


{\color{red}
Taken together, these observations indicate that our study should be viewed primarily as an applied instantiation of multi-task and transfer-learning principles in a gastric cancer setting, rather than as a proposal of a fundamentally new algorithm.
The backbone–head architecture, class-weighted loss, and fine-tuning strategy are intentionally simple and follow standard practice in deep learning for tabular EHR data~\citep{Caruana1997_MTL,Pan2010_TLsurvey,Yosinski2014_transferable}.
The added value lies in the systematic evaluation of cross-cancer pretraining, label-scarcity regimes, frozen-versus-full fine-tuning variants, and source-composition effects using only deployment-ready structured EHR variables.
}


\textcolor{red}{
From a clinical standpoint, the proposed framework is not meant to replace
diagnostic endoscopy, but to support triage and prioritization in routine
workflows.
Because it relies only on structured EHR variables that are already collected
in most hospitals, it could be implemented as a background risk score to flag
patients who may benefit from earlier endoscopic evaluation or closer
follow-up, particularly in systems where endoscopy capacity or specialist
access is limited.
Earlier identification of higher-risk patients in routine care could shorten
time-to-endoscopy and facilitate detection at more treatable stages, where
survival is markedly better~\citep{NCI2023stomach}.
Methodologically, the cross-cancer transfer design illustrates how related
malignancies can serve as source tasks to bootstrap prediction models for
cancers with few labels, a scenario that arises frequently in oncology
(e.g., rare tumors, early-stage subgroups, or low-incidence settings).
The transfer framework may therefore be appealing for institutions with few
confirmed GC cases but access to other GI/HPB cancer cohorts; in principle,
pretraining on related cancers could borrow strength and stabilize GC
performance, although this remains to be confirmed in external settings.
}


This study has several limitations.
%{\color{red}
%First, all experiments were conducted within a single U.S. tertiary-care hospital system using the MIMIC-IV database, and we did not perform external validation on independent EHR datasets. As a result, the generalizability of the proposed cross-cancer transfer learning framework to other health systems, care settings, and countries remains uncertain. Future work should include multi-center external validation and prospective evaluation to rigorously assess transportability and clinical impact.
%}
{
\color{red}
First, all experiments were conducted using MIMIC-IV, a de-identified,
single-center EHR database derived largely from inpatient encounters at a
U.S. tertiary academic medical center.
As a result, our models were trained and evaluated in a relatively
homogeneous institutional context, with shared clinical workflows,
laboratory assays, coding practices, and endoscopy referral thresholds.
Patient demographics, case mix, and access to care may differ substantially
in outpatient settings, community hospitals, or non-U.S. health systems,
leading to distributional shifts in both predictors and outcome prevalence.
Therefore, the absolute risk estimates and performance metrics reported here
should not be assumed to generalize to other institutions or outpatient
populations without further evaluation.
Real-world deployment will require rigorous external validation and, if
necessary, recalibration on independent multi-center and outpatient EHR
cohorts to assess robustness under domain shift and to quantify clinical
impact.
}
%First, MIMIC-IV is a single-center U.S. dataset derived largely from inpatient encounters; external validity to outpatient or multi-institutional settings remains unproven~\citep{Johnson2024MIMICIV}.
Second, cases were indexed by the first GC diagnosis code, and laboratory features were defined as the oldest available values within the 730 days preceding (and including) the index admission start date for cases (and across the full record for controls).
While this policy reduces peri-diagnostic bias relative to using recent values, it also collapses longitudinal information into single snapshots and may discard informative dynamics.
Moreover, restricting to a lab-complete cohort may introduce selection bias toward patients with more complete testing, potentially limiting generalizability to settings with sparser laboratory coverage.
\textcolor{red}{
In addition, because the held-out GC test set is of moderate size with a limited number of positive cases, the bootstrap confidence intervals are relatively wide and partially overlapping across models, which reflects finite-sample uncertainty.
Larger external cohorts will be required to narrow these intervals and to more precisely quantify the performance differences between models.
}
Finally, we did not evaluate subgroup fairness (e.g., by age, sex, or race/ethnicity) or conduct a full calibration analysis.

Several extensions appear promising.
(i) Move beyond static features to longitudinal encoders that incorporate trends, rates of change, and testing frequency.
(ii) Combine supervised multi-task pretraining with self-supervised objectives on large unlabeled EHR (e.g., masked feature modeling) to further improve sample efficiency. 
(iii) Explore domain adaptation and source reweighting to mitigate negative transfer when source–target mismatch is large.
(iv) Expand external validation across health systems.
%(v) Incorporate interpretability (e.g., post hoc attribution) to facilitate clinical review and identify spurious signals~\citep{park2024SHapley}.



\section{Conclusions}
By pretraining a shared encoder on related GI/HPB cancers and adapting it to gastric cancer (GC) using only routine and structured EHR variables,
{\color{red}in this single-center EHR cohort, this study illustrates a sample-efficient route to gastric cancer risk modeling under label scarcity and outlines a framework with potential for broader generalization, which will need to be confirmed in future multi-center external validation studies.}
%this study advances a generalizable and sample‑efficient route to cancer risk modeling under label scarcity.
Because it operates on data already collected in routine care, the approach can be embedded into EHR workflows to prioritize timely endoscopy referrals, and target follow‑up in resource‑constrained settings—aligning with the broader role of AI as clinician decision support in acute care. 
Before any clinical use, external multi‑site validation and prevalence‑aware calibration remain essential; nonetheless, the framework offers a practical blueprint for repurposing heterogeneous cancer cohorts to bootstrap GC models and, more broadly, a reusable foundation for early‑risk identification in other underrepresented cancers.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following supporting information can be downloaded at:  \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.}

% Only for journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title. A supporting video article is available at doi: link.}

% Only used for preprtints:
% \supplementary{The following supporting information can be downloaded at the website of this paper posted on \href{https://www.preprints.org/}{Preprints.org}.}

% Only for journal Hardware:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.\vspace{6pt}\\
%\begin{tabularx}{\textwidth}{lll}
%\toprule
%\textbf{Name} & \textbf{Type} & \textbf{Description} \\
%\midrule
%S1 & Python script (.py) & Script of python source code used in XX \\
%S2 & Text (.txt) & Script of modelling code used to make Figure X \\
%S3 & Text (.txt) & Raw data from experiment X \\
%S4 & Video (.mp4) & Video demonstrating the hardware in use \\
%... & ... & ... \\
%\bottomrule
%\end{tabularx}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization, D.H.; methodology, D.H., J.K. and J.J.; software, D.H., J.K. and J.J.; formal analysis, D.H., J.K. and J.J.; investigation, D.H., J.K. and J.J.; resources, D.H.; data curation, D.H., J.K. and J.J.; writing---original draft, D.H.; supervision, D.H.
All authors have read and agreed to the published version of the manuscript.}

\funding{This study was supported by the National R\&D Program for Cancer Control through the National Cancer Center(NCC) funded by the Ministry of Health \& Welfare, Republic of Korea(RS-2025-02264000).
This work was also supported by 2024 Research Fund of Myongji University.}

\institutionalreview{Ethical review and approval were waived for this study by the Institutional Review Board of Myongji University (IRB No. MJU 2025-09-001, approval date: 23 September 2025), because the research used only publicly available data (MIMIC-IV database) and did not involve the direct recruitment of human participants. The study was conducted in accordance with the Declaration of Helsinki.}

\informedconsent{Patient consent was waived because the study was an analysis of a third-party anonymized publicly available database.}

\dataavailability{
The datasets in this study are available through the MIMIC-IV database at \url{https://physionet.org/content/mimiciv/3.1/} (accessed on 1 October 2025) with the permission of PhysioNet.}

% Only for journal Drones
%\durcstatement{Current research is limited to the [please insert a specific academic field, e.g., XXX], which is beneficial [share benefits and/or primary use] and does not pose a threat to public health or national security. Authors acknowledge the dual-use potential of the research involving xxx and confirm that all necessary precautions have been taken to prevent potential misuse. As an ethical responsibility, authors strictly adhere to relevant national and international laws about DURC. Authors advocate for responsible deployment, ethical considerations, regulatory compliance, and transparent reporting to mitigate misuse risks and foster beneficial outcomes.}

% Only for journal Nursing Reports
%\publicinvolvement{Please describe how the public (patients, consumers, carers) were involved in the research. Consider reporting against the GRIPP2 (Guidance for Reporting Involvement of Patients and the Public) checklist. If the public were not involved in any aspect of the research add: ``No public involvement in any aspect of this research''.}
%
%% Only for journal Nursing Reports
%\guidelinesstandards{Please add a statement indicating which reporting guideline was used when drafting the report. For example, ``This manuscript was drafted against the XXX (the full name of reporting guidelines and citation) for XXX (type of research) research''. A complete list of reporting guidelines can be accessed via the equator network: \url{https://www.equator-network.org/}.}
%
%% Only for journal Nursing Reports
%\useofartificialintelligence{Please describe in detail any and all uses of artificial intelligence (AI) or AI-assisted tools used in the preparation of the manuscript. This may include, but is not limited to, language translation, language editing and grammar, or generating text. Alternatively, please state that “AI or AI-assisted tools were not used in drafting any aspect of this manuscript”.}

\conflictsofinterest{The authors declare no conflicts of interest.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional

%% Only for journal Encyclopedia
%\entrylink{The Link to this entry published on the encyclopedia platform.}

\abbreviations{Abbreviations}{
The following abbreviations are used in this manuscript:
\\

\noindent 
\begin{tabular}{@{}ll}
AP & Average precision\\
AUROC & Area under the receiver operating characteristic curve\\
TPR & True positive rate\\
FPR & False positive rate\\
EHR & Electronic health record\\
CBC & Complete blood count\\
CMP & Comprehensive metabolic panel\\
GI & Gastrointestinal\\
HPB & Hepatopancreatobiliary\\
GC & Gastric cancer\\
CC & Colorectal cancer\\
EC & Esophageal cancer\\
LC & Liver cancer\\
PC & Pancreatic cancer\\
LR & Logistic regression\\
MLP & Multilayer perceptron\\
XGB & Extreme Gradient Boosting
\end{tabular}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
\appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixstart
\appendix
\section[\appendixname~\thesection]{Hyperparameter Details}
\label{app:hparam}
\begin{table}[htbp]
\caption{Hyperparameter ranges.\label{tab:hparam-transfer-ofat}}
\centering
\begin{tabularx}{\textwidth}{CC}
\toprule
\textbf{Hyperparameter} & \textbf{Values} \\
\midrule
Architecture (layers) & \([128,64],\ [64,32],\ [32,16]\) \\
Dropout & \(0.2\) (fixed) \\
Learning rate in pretraining & \(10^{-2},\ 10^{-3},\ 10^{-4}\) \\
Pretraining weight decay & \(10^{-2},\ 10^{-3},\ 10^{-4}\) \\
Pretraining steps & \(1000,\ 2000,\ 5000\) \\
Learning rate in fine–tuning  & \(10^{-3},\ 10^{-4},\ 10^{-5}\) \\
Fine–tuning weight decay & \(10^{-3},\ 10^{-4},\ 10^{-5}\) \\
Fine–tuning epochs & \(20,\ 50,\ 100\) \\
\bottomrule
\end{tabularx}
\end{table}
We optimized hyperparameters using a one–factor–at–a–time (OFAT) sweep around a central configuration, selecting settings by mean validation AUROC over 10 repetitions.
Neural models used Adam~\cite{kingma2015adam};
mini-batch size was $\lfloor 0.05 \cdot N_c \rfloor$ capped to $[8,64]$ where $N_c$ is the data size for cancer type $c \in \{\CC,\EC,\LC,\PC,\GC\}$.
Complete ranges and the central configuration are provided in Table~\ref{tab:hparam-transfer-ofat}.
The meaning of some notations used in Table~\ref{tab:hparam-transfer-ofat} is as follows:
\begin{itemize}
\item \textit{Architecture.} A notation such as \([128,64]\) in Table~\ref{tab:hparam-transfer-ofat} denotes the MLP backbone architecture with two hidden layers of 128 and 64 units, respectively.
Each hidden block is implemented as \(\text{Affine-transformation}\rightarrow\text{Batch-normalization}\rightarrow\text{ReLU}\rightarrow\text{Dropout}\).
\item \textit{Dropout.} 0.2 of Dropout value in Table~\ref{tab:hparam-transfer-ofat} means that, during training, each hidden unit’s activation is independently set to zero with probability \(0.2\) (keep-probability \(0.8\)).
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\isPreprints{}{% This command is only used for ``preprints''.
\begin{adjustwidth}{-\extralength}{0cm}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
\bibliography{references}


% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
\reviewreports{\\
Reviewer 1 comments and authors’ response\\
Reviewer 2 comments and authors’ response\\
Reviewer 3 comments and authors’ response
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
%\isPreprints{}{% This command is only used for ``preprints''.
\end{adjustwidth}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
\end{document}

