\documentclass[11pt]{article}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{xr}

\externaldocument{gc-transfer}

\geometry{margin=1in}

\begin{document}

\begin{center}
{\Large \textbf{Responses to Reviewer 2}} \\[6pt]
%Manuscript ID: \textbf{diagnostics-3950051} \\
%Title: \textit{Cross-Cancer Transfer Learning for Gastric Cancer Risk Prediction from Electronic Health Records}
\end{center}

\vspace{1em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Summary}
We sincerely thank the reviewers for their careful reading of our manuscript and for providing insightful comments that helped us improve the clarity and rigor of the work. Below we provide detailed, point-by-point responses to all comments. All revisions are highlighted in the revised manuscript in \textcolor{red}{red}, and the locations of the changes are indicated in each response.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{}

%(p.~X, paragraph~Y, lines~A--B)

\subsection*{Comment 1}
\textbf{L46/7} Beyond framing multi-source data as “overlapping information,” authors should integrate clinical/biological context to strengthen the rationale for multi-source transfer.

\subsection*{Response 1}
We thank the reviewer for this helpful suggestion. In the revised manuscript, we have modified the Introduction to provide explicit clinical and biological rationale for why gastrointestinal and hepatopancreatobiliary cancers share EHR-visible signals that motivate multi-source pretraining (p.~2, lines~47--65).
%In particular, we now describe shared axes such as iron-deficiency anemia and occult bleeding across colorectal and gastric cancers, inflammatory and nutritional markers in liver disease and gastric cancer, and metabolic/metabolic-syndrome–related links between diabetes, colorectal, pancreatic, and gastric cancers, and connect these to our structured EHR feature set.
%We thank the reviewer for this helpful suggestion. In the revised manuscript, we have modified the Introduction to provide explicit clinical and biological rationale for why gastrointestinal and hepatopancreatobiliary cancers share EHR-visible signals that motivate multi-source pretraining (p.~X, paragraph~Y, lines~A--B).
%Specifically, we now describe: (i) how chronic occult gastrointestinal bleeding and iron-deficiency anemia (IDA) are shared early manifestations of colorectal and gastric cancers and are recommended triggers for endoscopic evaluation \citep{Hamilton2008CRC,Aksoy2019CBC,Krieg2024IDA,AGA2024IDA}; (ii) how anemia, systemic inflammation, and nutritional markers such as albumin and inflammatory ratios overlap across liver disease, primary liver cancer, and gastric cancer prognosis \citep{Stein2016Anemia,Gkamprela2017CLDIDA,Crumley2010Albumin,Kim2020Inflam}; and (iii) how metabolic syndrome and dysglycemia are associated with colorectal, pancreatic, and gastric cancer risk \citep{MetS_CRC_2021,Zhong2023MetS_PC,Shimoyama2013DMGC,Yoon2013DMGC,Guo2022DMGC}.
%We then connect these shared pathophysiologic axes to the structured EHR variables used in our model (CBC indices, basic chemistries, and ICD-derived comorbidities), clarifying why multi-cancer pretraining can learn transferable representations for gastric cancer risk.

\subsection*{Comment 2}
\textbf{L206} While Table 2, Table 3, Figure 1, and Figure 2. report the mean performance metrics (e.g., AUROC, AP), they lack error bars across repetitions,which can fully assess the consistency and statistical significance of the transfer learning gains.

\subsection*{Response 2}
We thank the reviewer for this important suggestion and have revised the manuscript to explicitly quantify uncertainty, stability across repetitions, and the statistical significance of the transfer learning gains.

\begin{itemize}
\item \textit{Statistical Analysis (Section~2.4).}
We have expanded the description of our evaluation protocol to clarify that
(i) all models are trained and evaluated over 10 independent repetitions with different random splits,
(ii) we report standard deviations or 95\% confidence intervals across these repetitions, and
(iii) we compute 95\% confidence intervals for AUROC using patient-level bootstrap resampling of the test set (2000 resamples), and apply DeLong’s test for pairwise AUROC comparisons between the Transfer model and each non-transfer baseline (LR, XGB, scratch MLP).
These additions are highlighted in red in the revised \textit{Statistical Analysis} section
(p.~7, lines~266--273).

\item \textit{Tables~\ref{tab:model-compare-gc1} and Table~\ref{tab:transfer-by-pretrain-types}.}
In Table~\ref{tab:model-compare-gc1}, we now present AUROC as the mean test performance across repeated runs with the standard deviation ($\pm$) and the 95\% bootstrap confidence interval in parentheses.
In Table~\ref{tab:transfer-by-pretrain-types}, we likewise report AUROC as mean~$\pm$~standard deviation across repetitions.
These revisions appear in the \textit{Results} section.
% where Tables~\ref{tab:model-compare-gc1} and \ref{tab:transfer-by-pretrain-types} are introduced
%(\textit{p.~\textbf{XX}, paragraph~1, lines~\textbf{YYY}--\textbf{ZZZ}} for Table~\ref{tab:model-compare-gc1};
%\textit{p.~\textbf{XX}, paragraph~1, lines~\textbf{YYY}--\textbf{ZZZ}} for Table~\ref{tab:transfer-by-pretrain-types}).

\item \textit{Figure~\ref{fig:model-gcrate} and Figure~\ref{fig:transfer-combo-gcrate}.}
For Figure~\ref{fig:model-gcrate} and Figure~\ref{fig:transfer-combo-gcrate}, we added error bars that represent the 95\% confidence intervals across repetitions for each model and GC label fraction.
%The figure captions were updated to state:
%\emph{``Error bars denote 95\% confidence intervals across repetitions.''}
%These changes are reflected in the figure captions in the \textit{Results} section
%(\textit{p.~\textbf{XX}, caption of Figure~\ref{fig:model-gcrate}, lines~\textbf{YYY}--\textbf{ZZZ}} and
%\textit{p.~\textbf{XX}, caption of Figure~\ref{fig:transfer-combo-gcrate}, lines~\textbf{YYY}--\textbf{ZZZ}}).

\item \textit{Results section: variability, confidence intervals, and DeLong’s test.}
In Section~3.2, we now explicitly summarize the variability and confidence intervals across repetitions, noting that:
(i) the standard deviations of test AUROC are small for all models (indicating stable estimates),
(ii) the 95\% bootstrap confidence interval for the Transfer model is shifted upward relative to that of the scratch MLP (with partial overlap due to the moderate size of the GC test set), and
(iii) pairwise DeLong’s test on the common GC test set shows that the Transfer model achieves significantly higher AUROC than LR and XGB ($p = 0.0023$ and $p = 0.0433$, respectively), whereas the improvement over the scratch MLP does not reach conventional significance ($p = 0.1231$), consistent with a modest effect size and finite-sample uncertainty.
These additions are highlighted in red in the \textit{Comparison of Models} subsection
(pp.~8--9, lines~327--342).

\item \textit{Results section: stability across GC label fractions.}
In Section~3.3, we further comment on the repetition-level confidence intervals in Figure~\ref{fig:model-gcrate}, emphasizing that the Transfer model maintains relatively narrow and nearly constant intervals across GC sampling rates, whereas LR, XGB, and the scratch MLP show wider dispersion under label scarcity.
This provides an explicit assessment of model stability across repeated experiments.
The corresponding sentences are highlighted in red in the \textit{Model performance under reduced GC training labels} subsection
(p.~10, lines~369--374).

\item \textit{Discussion section: interpretation of uncertainty and effect size.}
In Section~4, we now explicitly interpret these uncertainty estimates, stating that
(i) the gains of the Transfer model are modest in absolute magnitude but stable across repetitions and supported by bootstrap CIs and DeLong’s test, and
(ii) the relatively wide and partially overlapping CIs mainly reflect finite-sample uncertainty in the moderately sized GC test cohort.
We also discuss that pretraining acts as an implicit regularizer that yields tighter and more stable confidence intervals for the Transfer model under label scarcity.
These revisions are highlighted in red in the \textit{Discussion} section
(p.~13, lines~457--471).
\end{itemize}




%We hope that these revisions better communicate why this cross-cancer transfer
%framework is relevant and of interest to both clinical and methodological
%audiences.

\subsection*{Comment 3}
L353 For a clinical risk prediction tool, model calibration and subgroup fairness are essential for safe deployment.

\subsection*{Response 3}
We fully agree that well-calibrated risk estimates and subgroup fairness are crucial for safe clinical deployment.
In response to this comment, we have added an explicit calibration analysis in the revised manuscript,
including reliability diagrams and expected calibration error (ECE) for all four models on the held-out GC test set
(Figure~\ref{fig:reliability-gc1}; p.~9, lines~344--352; p.~13, lines~472--476).
These results complement our discrimination metrics by characterizing the agreement between predicted and observed risks.
We also expanded the Discussion and Limitations to clarify that, while we now report aggregate calibration,
we have not yet performed a systematic subgroup fairness evaluation, and that future work should assess calibration
and performance across key patient subgroups and external cohorts prior to any clinical deployment
(p.~15, lines~548--551; p.~16, lines~601--607).
%We agree that well-calibrated risk estimates and subgroup fairness are crucial for safe clinical deployment.
%In the revised manuscript, we have (i) added an explicit statement in the Discussion highlighting the need for
%careful assessment of calibration and subgroup fairness prior to deployment
%(p.~X, paragraph~Y, lines~A--B), and (ii) expanded the Limitations to note that our current analysis is restricted
%to aggregate calibration on the GC test set and does not yet include subgroup fairness evaluation
%(p.~X, paragraph~Y, lines~A--B).



\subsection*{Comment 4}
\textbf{L364} The authors conclude that their framework offers a “generalizable... route to cancer risk modeling”. However, this claim of generalizability is premature and not yet supported by the evidence, as all experiments are confined to a single dataset. To maintain scientific rigor, the conclusion should be tempered. I strongly recommend rephrasing to clarify that the approach has the potential to be generalizable, but that this must be rigorously tested in future multi-center studies. The absence of external validation is the primary barrier to claiming generalizability at this stage.

\subsection*{Response 4}
We thank the reviewer for carefully highlighting the issue of external validation.
We fully agree that the lack of external validation is a major limitation and that it restricts the generalizability of our findings beyond the MIMIC-IV cohort.
At present, we do not have access to an independent multi-center or outpatient EHR dataset under appropriate data use agreements, and we regret that, within the constraints of this revision, we were not able to carefully design and evaluate a domain-shift emulation (e.g., cross-year or stratified splits that reflect real differences in practice patterns) that would provide a robust assessment of transportability.
In light of these practical constraints, we strengthen the manuscript by more clearly articulating this limitation and its implications for deployment.

Specifically, in the Discussion we now (i) describe MIMIC-IV more explicitly as a single-center, predominantly inpatient database and discuss how differences in patient mix, workflows, and practice patterns may limit generalizability to outpatient and multi-institutional settings, and (ii) emphasize that the performance estimates reported here should not be assumed to transfer to other institutions without rigorous external validation and, if necessary, recalibration (p.~15, lines~553--564).
We also revised the Conclusions to frame our work as a proof-of-concept demonstration within a single-center cohort and to state clearly that real-world deployment will require validation on independent multi-center and outpatient EHR cohorts to assess robustness under domain shift (p.~16, lines~616--621).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliographystyle{plainnat}
%\bibliography{references} % Replace ‘references.bib’ with your bib file name.

\end{document}
