%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[journal,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aieduc, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, amh, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, glacies, grasses, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, iic, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdad, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joitmc, joma, jop, jor, journalmedia, jox, jpbi, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidney, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, benchmark, book, bookreview, briefcommunication, briefreport, casereport, changes, clinicopathologicalchallenge, comment, commentary, communication, conceptpaper, conferenceproceedings, correction, conferencereport, creative, datadescriptor, discussion, entry, expressionofconcern, extendedabstract, editorial, essay, erratum, fieldguide, hypothesis, interestingimages, letter, meetingreport, monograph, newbookreceived, obituary, opinion, proceedingpaper, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, supfile, systematicreview, technicalnote, viewpoint, guidelines, registeredreport, tutorial,  giantsinurology, urologyaroundtheworld
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Title}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Title}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0000-0000-000X} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-0000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Firstname Lastname $^{1}$\orcidA{}, Firstname Lastname $^{2}$ and Firstname Lastname $^{2,}$*}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Firstname Lastname, Firstname Lastname and Firstname Lastname}

% MDPI internal command: Authors, for citation in the left column, only choose below one of them according to the journal style
% If this is a Chicago style journal 
% (arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci): 
% Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% If this is a APA style journal 
% (admsci, behavsci, businesses, econometrics, economies, education, ejihpe, games, humans, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth): 
% Lastname, F., Lastname, F., \& Lastname, F.

% If this is a ACS style journal (Except for the above Chicago and APA journals, all others are in the ACS format): 
% Lastname, F.; Lastname, F.; Lastname, F.
\isAPAStyle{%
       \AuthorCitation{Lastname, F., Lastname, F., \& Lastname, F.}
         }{%
        \isChicagoStyle{%
        \AuthorCitation{Lastname, Firstname, Firstname Lastname, and Firstname Lastname.}
        }{
        \AuthorCitation{Lastname, F.; Lastname, F.; Lastname, F.}
        }
}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Affiliation 1; e-mail@e-mail.com\\
$^{2}$ \quad Affiliation 2; e-mail@e-mail.com}

% Contact information of the corresponding author
\corres{Correspondence: e-mail@e-mail.com; Tel.: (optional; include country code; if there are multiple corresponding authors, add author initials) +xx-xxxx-xxx-xxxx (F.L.)}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation.}  
% Current address should not be the same as any items in the Affiliation section.

%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes.

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{A single paragraph of about 200 words maximum. For research articles, abstracts should give a pertinent overview of the work. We strongly encourage authors to use the following style of structured abstracts, but without headings: (1) Background: place the question addressed in a broad context and highlight the purpose of the study; (2) Methods: describe briefly the main methods or treatments applied; (3) Results: summarize the article's main findings; (4) Conclusions: indicate the main conclusions or interpretations. The abstract should be an objective representation of the article, it must not contain results which are not presented and substantiated in the main text and should not exaggerate the main conclusions.}

% Keywords
\keyword{keyword 1; keyword 2; keyword 3 (List three to ten pertinent keywords specific to the article; yet reasonably common within the subject discipline.)} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal BioTech, Fishes, Neuroimaging and Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Advances in Respiratory Medicine, Future, Sensors and Smart Cities
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%
%
%\noindent This is an obligatory section in ``Advances in Respiratory Medicine'', ``Future'', ``Sensors'' and ``Smart Cities”, whose goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\section{Introduction}
Gastric cancer (GC) continues to pose a major challenge to global health, being the fifth most frequently diagnosed cancer and the fourth most common cause of cancer-related death worldwide \cite{sung2021global}. 
Harnessing routinely collected EHR data for earlier GC identification could improve outcomes by enabling timely endoscopy and treatment.
Although structured EHR signals (demographics, diagnoses, laboratories) are attractive for scalable screening,
however, EHR data suitable for model development may be hard to collect due to multiple reasons: incomplete or missing records, irregular sampling and follow-up patterns, and the overall complexity of clinical documentation.
Moreover, ethical and regulatory requirements—such as the necessity of obtaining patient consent or institutional approvals—can further restrict the availability of usable data.
As a result, the number of patients that can be effectively included for model training may remain limited.
% TODO: citation

Transfer learning provides a strategy to mitigate data scarcity by reusing knowledge learned on related tasks.
In EHR oncology, phenotypes across gastrointestinal and hepatopancreatobiliary cancers can share risk factors, laboratory patterns, and diagnosis trajectories.
We therefore explore a cross-cancer pretraining paradigm: train a shared backbone on non-gastric cancers and adapt it to GC via fine-tuning.
We evaluate this hypothesis in the MIMIC-IV v3.1 critical care database \citep{Johnson2024MIMICIV}, building models that use only structured data to avoid features that merely encode physician suspicion.

%Our contributions are threefold:
%(i) a multi-head transfer MLP with structural-parameter (L2-SP) regularization \citep{li2018l2sp} and controllable fine-tuning heads (including logit averaging) to stabilize adaptation; and
%(ii) a rigorous, seed-repeated selection protocol that fixes model choice by validation AUROC and then reports both validation and held-out test metrics with sensitivity/specificity/F1 at a validation-optimized threshold \citep{davis2006relationship,saito2015precision}.

%To test this hypothesis, we developed a reproducible pipeline on the MIMIC-IV v3.1 database to construct cohorts, engineer biologically meaningful features, and train several baseline models along with a transfer-learning multilayer perceptron (Transfer-MLP). The Transfer-MLP is pretrained on non-GC GI cancers (colorectal, liver, esophageal, pancreatic) and then fine-tuned on GC labels using a controlled head-selection strategy and structural parameter (SP) regularization. Our contributions are: (i) a transparent, fully code-driven cohort construction and feature engineering workflow that avoids features merely encoding physicians' suspicion; (ii) a multi-head transfer architecture that supports different fine-tuning strategies; and (iii) an empirical demonstration that pretraining on related cancers improves early GC prediction compared with GC-only training.


{\color{red}
Gastric cancer (GC) continues to pose a major challenge to global health, being the fifth most frequently diagnosed cancer and the fourth most common cause of cancer-related death worldwide \citep{sung2021global}. Harnessing routinely collected electronic health record (EHR) data for earlier GC identification could improve outcomes by enabling timely endoscopy and treatment.

In practice, however, assembling EHR datasets that are truly usable for model development can be difficult. For GC in particular, early-stage cases are relatively uncommon in routine care pathways; clinical records can be incomplete or missing; follow-up and sampling are often irregular; and clinical documentation is complex and heterogeneous. In addition, ethical and regulatory safeguards---for example, the need for institutional approvals and, in some settings, patient consent---can further constrain the availability of analyzable patient-level data. As a result, the number of patients that can be effectively included for model training may remain limited.

Transfer learning offers a principled way to mitigate data scarcity by reusing structure learned from related tasks. In gastrointestinal oncology, non-gastric cancers (e.g., colorectal, liver, esophageal, pancreatic) share overlapping comorbidity profiles and laboratory patterns. We therefore explore a cross-cancer pretraining paradigm: train a shared backbone on non-gastric cancers and adapt it to GC via fine-tuning, using only structured EHR features so as not to rely on signals that merely encode physician suspicion. We evaluate this hypothesis in the MIMIC-IV v3.1 critical care database \citep{Johnson2024MIMICIV}.
}

\section{Related Work}
{
\color{red}
Population-scale screening and claims cohorts have enabled the development of GC risk models using noninvasive, routinely available variables. Using the Korean National Health Insurance Service screening data (2013--2014) with 10{,}515{,}949 adults and 5-year follow-up, Park et~al.\ trained several algorithms and found that logistic regression achieved the highest discrimination (internal AUROC 0.708; external AUROC 0.669); they further applied SHAP to highlight age, sex, \textit{Helicobacter pylori} infection, smoking, and family history as the top contributors \citep{park2024crt}. Class imbalance was addressed via ROSE oversampling, and multiple imputation (MICE) supported robustness analyses \citep{park2024crt}.

A complementary line of work develops EHR-only models that avoid endoscopy/pathology so they can be deployed to broader US populations. Kim et~al.\ built a multivariable logistic regression model for noncardia GC (NCGC) using a large multi-state EHR; after multiple imputation, the 0.632 estimator of AUC was 0.731. They reported threshold-dependent operating points (e.g., specificity $\approx90\%$ at PPV $\approx1\%$) and discussed pragmatic EHR limitations such as missingness and the poor capture of key risk factors (e.g., H.\ pylori, alcohol use) \citep{kim2024gastha}.

Methodologically, the relative merits of classical models vs.\ machine learning on EHR have been investigated. In two independent EHR systems (Stanford, University of Washington), Huang et~al.\ compared logistic regression to lasso, SVM, k-nearest neighbors, and random forests for NCGC risk classification. Across internal and external validation, logistic regression performed comparably to optimized ML models, underscoring the value of well-specified, interpretable models when data are heterogeneous and partially missing \citep{huang2022jco}. They also detailed the practical burden of phenotyping from unstructured notes and the sensitivity of results to feature availability \citep{huang2022jco}.

Our study differs from the above in two ways. First, we do not rely on endoscopic or pathology-derived features, focusing instead on structured EHR signals. Second, we leverage a transfer-learning strategy that \emph{pretrains} on non-gastric gastrointestinal cancers and \emph{fine-tunes} on GC, thereby exploiting cross-cancer regularities to improve representation learning under GC data constraints.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials and Methods}
\subsection{Data Source and Study Design}
We used the publicly available MIMIC-IV (version 3.1) database. We assembled five disease cohorts: GC (target) and four related cancers---colorectal (C), liver (L), esophageal (E), and pancreatic (P). For each cancer, we identified cases by the earliest inpatient diagnosis timestamp meeting curated ICD-9/ICD-10 prefix lists. For each case, we sampled three controls (1:3 ratio) from patients without the corresponding cancer code, using demographic and temporal constraints to reduce bias. Controls were assigned an index date aligned to the case reference to prevent leakage. The default configuration used in this study enabled laboratory features and disabled date-window matching to increase sample size.




\subsection{Preprocessing, Splits, and Normalization}
For each cancer, we concatenated its cases and matched controls to create a full dataset, dropped ignore-listed columns (subject identifier, index timestamp, and time-gap features), performed stratified train/test splitting (85\%/15\%), and further carved a validation split from training data when cross-validation was disabled. Continuous features were standardized using parameters fit on the training partition and applied to validation and test sets.


\subsection{Models and Training Protocols}
\paragraph{Baselines.} We trained (a) logistic regression with $\ell_2$ regularization and optional class weighting; (b) gradient-boosted trees (XGBoost) with typical depth, learning rate, and regularization settings; and (c) a multilayer perceptron (MLP) with batch normalization and dropout. We used class-balanced BCE-with-logits losses with a positive-class weight consistent with the case:control ratio.


\paragraph{Transfer-MLP.} The transfer architecture is a shared backbone with a final linear layer producing one logit per pretraining cancer type. Pretraining optimizes binary cross-entropy per head using only the matched head for each mini-batch. Three fine-tuning strategies were supported: (i) \texttt{head\_mode=new} adds a new GC head; (ii) \texttt{best} selects the single best pretraining head by validation AUROC/AP and fine-tunes that head; and (iii) \texttt{avg} averages logits over all pretraining heads and fine-tunes with a scalar output. Fine-tuning optionally freezes the backbone (head-only) or updates all parameters. To preserve useful pretrained representations, we employed structural parameter regularization (L2-SP) that penalizes deviation from the pretrained weights of select linear layers. Mini-batch size could follow a fixed value or adapt to dataset size via a proportion parameter.


\paragraph{Early stopping and selection.} For baselines and fine-tuning, model selection on the validation split was based on AUROC (primary) with AP as secondary. When enabled, early stopping restored the best-performing weights. For pretraining with multi-type loaders, validation was performed either per type or by averaging across types, depending on the loader strategy.


\subsection{Evaluation Metrics}
We report AUROC and average precision (AP) as threshold-free metrics. At an operating point optimized for F1 on the validation data, we also report sensitivity (recall), specificity, and F1 on validation and test splits. Thresholds were obtained from the precision--recall curve.



\section{Materials and Methods (v1)}

\subsection{Data Source and Ethics}

We used the de-identified MIMIC-IV clinical database (v3.x) curated by MIT-LCP and collaborators~\citep{Johnson2023_MIMICIV}. Data access requires credentialing and a data-use agreement through PhysioNet. Because MIMIC-IV is de-identified, this secondary analysis was deemed exempt from institutional review; no direct patient contact occurred.

\subsection{Cohort Construction and Labels}

Let $\mathcal{D}$ denote all eligible hospitalizations in MIMIC-IV. We identified incident cancer cases using standard ICD-9/10 code prefixes. Specifically, gastric cancer (GC) codes defined positive labels ($y{=}1$), and non-GC cancers used for pretraining included colorectal (C), liver (L), esophageal (E), and pancreatic (P) groups. For each cancer type, we extracted the earliest diagnosis time $t_c$ per subject. Controls ($y{=}0$) were sampled at a fixed case:control ratio of $1{:}3$ while ensuring no cancer codes prior to the index time; when date matching was enabled, controls had at least one diagnosis within $\pm D$ days of each corresponding case index date.

Features included (i) demographics (age, sex), (ii) binary comorbidity indicators derived from ICD code families (e.g., diabetes, GERD, anemia), and (iii) the most recent routine laboratory tests within a \mbox{$\pm 30$-day} window around the index time (e.g., hemoglobin, creatinine, electrolytes). Missing laboratory values were left as \texttt{NaN} and handled implicitly by the downstream models or imputers, consistent with our implementation. Formally, each subject is represented by $(x_i,y_i)$ with $x_i\in\mathbb{R}^d$ and $y_i\in\{0,1\}$.

\subsection{Train/Validation/Test Partitions}

For each cancer type, subjects were partitioned into non-overlapping train/validation/test sets to avoid subject leakage. When cross-validation was enabled, stratified folds preserved the case--control ratio within each split~\citep{Pedregosa2011_sklearn}. Random seeds were fixed for reproducibility.

\subsection{Models}

We considered three baselines and one transfer model:

\paragraph{Logistic Regression (LR).} Given standardized features $\tilde{x}=(x-\mu)/\sigma$, LR learns $p_\theta(y{=}1\mid x)=\sigma(w^\top \tilde{x}+b)$, where $\sigma(\cdot)$ is the logistic sigmoid and $\theta{=}(w,b)$. Parameters are estimated by minimizing class-weighted cross-entropy.

\paragraph{Gradient Boosting (XGBoost).} We used XGBoost with scale-sensitive weighting to address imbalance~\citep{Chen2016_XGBoost}.

\paragraph{Multilayer Perceptron (MLP).} A feed-forward network $f_\phi:\mathbb{R}^d\to\mathbb{R}$ with hidden layers, batch normalization, ReLU activations, and dropout~\citep{Ioffe2015_BN,Srivastava2014_Dropout}.

\paragraph{Transfer MLP (Multi-Head).} We parameterize a shared backbone $f_\phi:\mathbb{R}^d\!\to\!\mathbb{R}^h$ and cancer-specific linear heads $\{h_c:\mathbb{R}^h\!\to\!\mathbb{R}\}_{c\in\mathcal{C}}$ producing logits $z_c(x)=h_c(f_\phi(x))$. During pretraining on non-GC types $\mathcal{C}{=}\{C,E,L,P\}$, the objective is
\begin{equation}
\label{eq:pretrain}
\min_{\phi,\{h_c\}}\;\; \mathbb{E}_{(x,y,c)}\Big[\; \ell_{\mathrm{BCE}}(z_c(x),y)\;\Big],
\end{equation}
where $\ell_{\mathrm{BCE}}$ denotes class-weighted binary cross-entropy.

At fine-tuning on GC, we reuse the pretrained $(\phi^\star,\{h_c^\star\})$ and choose one of three head strategies:
\begin{align}
\textbf{avg: } & s(x)=\tfrac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}} z_c(x), \nonumber\\
\textbf{best: } & s(x)=z_{\hat{c}}(x), \;\; \hat{c}=\arg\max_{c\in\mathcal{C}}\mathrm{AUROC}_c\; \text{on validation}, \nonumber\\
\textbf{new: } & s(x)=z_G(x), \;\; \text{add a new GC head } h_G. \nonumber
\end{align}
We optionally freeze the backbone (\texttt{ft\_head\_only}) or fine-tune it jointly. An L2-SP penalty~\citep{Li2018_L2SP} anchors trainable parameters to their pretrained values:
\begin{equation}
\label{eq:l2sp}
\Omega_{\mathrm{L2\text{-}SP}}(\theta)=\!\!\sum_{j\in \mathcal{S}}\!\!\|\theta_j-\theta_j^\star\|_2^2,
\end{equation}
where $\mathcal{S}$ indexes selected weights (biases excluded; the last layer can be excluded in \texttt{new} mode). The fine-tuning objective is then
\begin{equation}
\label{eq:finetune}
\min_{\theta}\;\; \mathbb{E}_{(x,y)\sim \text{GC}}\!\Big[\ell_{\mathrm{BCE}}(s(x),y)\Big] \;+\; \lambda\, \Omega_{\mathrm{L2\text{-}SP}}(\theta).
\end{equation}

\subsection{Losses, Class Imbalance, and Thresholding}

For a logit $u$ and label $y\!\in\!\{0,1\}$, the weighted BCE used throughout is
\begin{equation}
\ell_{\mathrm{BCE}}(u,y)= -\alpha\, y\log\sigma(u) - (1-y)\log\big(1-\sigma(u)\big),
\end{equation}
with positive-class weight $\alpha$ set to the case:control ratio (here $1{:}3$). Decision thresholds are chosen on the validation set by maximizing the $F_1$-score,
\begin{equation}
t^\star=\arg\max_{t\in[0,1]}\; F_1\!\big(t\big)=\frac{2\,\mathrm{Precision}(t)\cdot\mathrm{Recall}(t)}{\mathrm{Precision}(t)+\mathrm{Recall}(t)}.
\end{equation}

\subsection{Optimization and Early Stopping}

Neural networks were optimized with Adam~\citep{Kingma2015_Adam} using mini-batches. Batch normalization and dropout act as regularizers~\citep{Ioffe2015_BN,Srivastava2014_Dropout}. Early stopping on validation AUROC was optionally enabled.\footnote{Implementation details, including the exact placement of batch normalization and dropout layers and the early-stopping switch, follow the released Python code.}

\subsection{Evaluation Metrics}

We report the area under the receiver operating characteristic curve (AUROC) and the average precision (AP, area under the precision--recall curve), along with sensitivity and specificity computed at $t^\star$. In imbalanced settings, AP is often more discriminative than AUROC~\citep{Saito2015_PR}.

\subsection{Implementation and Reproducibility}

Experiments are implemented in Python/PyTorch. The end-to-end entry point is \texttt{main30\_4\_2.py}, which prepares data, builds per-setting configurations, and runs models. Pretraining and fine-tuning are encapsulated in modular trainers; configuration management ensures deduplication of runs. The software supports parallel sweeps, deterministic seeding, and a graceful stop mechanism via a file flag. Hyperparameters (e.g., learning rates, weight decay, depth, dropout, \texttt{head\_mode}, L2-SP toggle, mini-batch rate) are exposed through a grid for reproducible search.\footnote{All implementation choices described in this section are traceable to the provided source code files in the submission package: data preprocessing and cohort construction; model training for LR/MLP/XGBoost; transfer-learning trainer with multi-head strategies and L2-SP; configuration management and parameter grid utilities; and the orchestrator script.}



\section{Materials and Methods (v2)}

\subsection{Data Source and Ethics}
We used the publicly available de-identified MIMIC-IV clinical database (v3.x) curated by MIT-LCP~\citep{Johnson2023_MIMICIV}.
Access to the database was granted to our research team, and we adhered to all usage agreements, including the prohibition of data disclosure and misuse.

\subsection{Cohort Construction and Features}
In this study, we constructed cohorts not only for gastric cancer (GC) but also for multiple non-GC cancers (colorectal, esophageal, liver, and pancreatic).
This design emphasizes that our approach leverages diverse cancer types for pre-training before fine-tuning on GC.
For each cancer, we identified cases by the earliest qualifying ICD-9/10 diagnosis (prefix lists for GC and four non-GC cancers) and sampled controls at a fixed $1{:}3$ case:control ratio, excluding subjects with the target cancer.
The GC index time $t_G$ is the first GC diagnosis; for non-GC cancers $c\in\{\mathrm{C},\mathrm{E},\mathrm{L},\mathrm{P}\}$ we analogously define $t_c$.
Features include demographics (age, sex), binary ICD-derived comorbidity indicators (e.g., diabetes, GERD, ulcers), and the most recent values of routine labs (CBC/chemistry) in a $\pm 30$-day window around the index time (or all available values for controls).
Identical preprocessing is applied to cases and controls; numerical features are standardized by removing the mean and scaling to unit variance using statistics on the training split only.
To address missing values, we applied median imputation based on the control population. 
For each feature, the median value observed among the controls was calculated and substituted for missing entries across both cases and controls. 
This strategy minimizes information leakage from the case group and provides a robust central tendency measure for imputation.
[[TODO: finally used feature list ]]


\subsection{Cohort Construction and Features}

\paragraph{Case--control cohorts and index times.}
For each cancer, we identified cases by the earliest qualifying ICD-9/10 diagnosis (prefix lists for GC and four non-GC cancers) and sampled controls at a fixed $1{:}3$ case:control ratio, excluding subjects with the target cancer. The GC index time $t_G$ is the first GC diagnosis; for non-GC cancers $c\in\{\mathrm{C},\mathrm{E},\mathrm{L},\mathrm{P}\}$ we analogously define $t_c$.

\paragraph{Feature engineering (common across cancers).}
We constructed features identically for cases and controls: (i) demographics, (ii) binary indicators from ICD-9/10 groupings, and (iii) routine laboratory values. For laboratory variables, we first restricted rows to a curated itemid list and then, for each subject and item, took the \emph{last available value} within a $\pm30$\,day window around the index time for cases; for controls (when date matching was disabled), the last available value across all observations was used. Missing laboratory values were then aligned to the index time as described and missing entries imputed in a consistent manner across cohorts; laboratory-missingness flags, when created during preprocessing, were removed prior to modeling.

\paragraph{Final feature set used for modeling.}
After removing identifier/time columns and a small list of prespecified exclusions, the final input vector $x\in\mathbb{R}^d$ comprised:
\begin{itemize}[leftmargin=12pt,topsep=2pt,itemsep=1pt]
  \item \textbf{Demographics} (2): \texttt{AGE} (years), \texttt{MALE} (0/1).
  \item \textbf{ICD-derived binary indicators} (26): \texttt{DIABETES}, \texttt{OBESITY}, \texttt{ALCOHOL}, \texttt{SMOKE}, \texttt{GERD}, \texttt{DYSPEPSIA}, \texttt{ULCER}, \texttt{GASTRITIS}, \texttt{HPYLORI}, \texttt{HYPERTENSION}, \texttt{MI\_AP}, \texttt{DYSLIPIDEMIA}, \texttt{GA}, \texttt{FAMILY\_HISTORY\_CANCER}, \texttt{PROTEIN\_CALORIE\_MALNUTRITION}, \texttt{POSTHEMORRHAGIC\_ANEMIA}, \texttt{ADVERSE\_EFFECT\_ANTINEOPLASTIC}, \texttt{DYSPHAGIA}, \texttt{ANTINEOPLASTIC\_CHEMOTHERAPY}, \texttt{ATRIAL\_FIBRILLATION}, \texttt{IRON\_DEFICIENCY\_ANEMIA}, \texttt{KIDNEY\_FAILURE}, \texttt{CORONARY\_ATHEROSCLEROSIS\_NATIVE}, \texttt{ATHEROSCLEROTIC\_HEART\_DISEASE\_NATIVE\_NO\_ANGINA}, \texttt{PURE\_HYPERCHOLESTEROLEMIA}, \texttt{HYPO\_OSMOLALITY\_HYPONATREMIA}.
  \item \textbf{Laboratory variables} (22): \texttt{HGB}, \texttt{MCV}, \texttt{Bicarbonate}, \texttt{Chloride}, \texttt{Potassium}, \texttt{Sodium}, \texttt{Urea\_Nitrogen}, \texttt{Creatinine}, \texttt{Glucose}, \texttt{Anion\_Gap}, \texttt{Red\_Blood\_Cells}, \texttt{White\_Blood\_Cells}, \texttt{Hematocrit}, \texttt{MCHC}, \texttt{Platelet\_Count}, \texttt{RDW}, \texttt{MCH}, \texttt{Basophils}, \texttt{Eosinophils}, \texttt{Lymphocytes}, \texttt{Monocytes}, \texttt{Neutrophils}.
\end{itemize}

\subparagraph{Standardization of numerical features.}
Let $\mathcal{N}$ denote the standardized numerical variables,
\(\mathcal{N}=\{\texttt{AGE}\}\cup\{\text{the 22 laboratory variables listed above}\}.\)
We fit a single StandardScaler~\citep{Pedregosa2011_sklearn} on the \emph{training split only} (before carving out validation from the training set) and apply it to training and test; the validation split is then taken as a stratified subset of the already-standardized training data. For $j\in\mathcal{N}$,
\begin{equation}
\tilde{x}_j=\frac{x_j-\hat{\mu}_j}{\hat{\sigma}_j},\qquad
\hat{\mu}_j=\frac{1}{n_{\text{train}}}\sum_{i\in \text{train}}x_{ij},\ \
\hat{\sigma}_j^2=\frac{1}{n_{\text{train}}}\sum_{i\in \text{train}}(x_{ij}-\hat{\mu}_j)^2.
\end{equation}



%\subsection{Feature Engineering}
%We engineered features that are interpretable and clinically plausible. Demographics included age and sex. Binary ICD-derived comorbidity indicators were computed for clinically relevant conditions (e.g., diabetes, obesity, GERD, dyspepsia, gastric ulcer, alcohol/tobacco use, anemia, malnutrition, atrial fibrillation, renal failure, lipid disorders). For laboratory features, we included the most recent value per analyte within a symmetric look-back window around the index date (or all available values for controls), covering complete blood count indices and common chemistries (e.g., hemoglobin, hematocrit, MCV/MCH/MCHC, WBC and differentials, platelets, RDW; sodium, potassium, chloride, bicarbonate, urea nitrogen, creatinine, glucose, anion gap). Missing values were imputed and missingness flags optionally dropped after imputation. Non-predictive index-related variables were excluded from modeling.

\subsection{Train/Validation/Test Partitions}
For each cancer type, we first divided the subjects into training and test sets, using an 85\%--15\% split. 
We further split the training set so that 20\% of it was used as a validation set. 
In all splits, stratified sampling was performed to ensure that the proportion of cases and controls remained consistent across the training, validation, and test sets. 
Feature standardization was carried out by computing the mean and standard deviation from the training set, and the same parameters were then applied to both the validation and test sets.

For each cancer, we stratified subjects into train/test (85/15\%) and further carved a validation set (20\% of train) when cross-validation was disabled. Stratification preserved the case--control ratio in each split. Standardization parameters were fit on the training data and applied to validation and test.

\subsection{Models}

\paragraph{Logistic Regression (LR).} With standardized features $\tilde{x}=(x-\mu)/\sigma$, LR models $p_\theta(y = 1\mid x)=\sigma(w^\top \tilde{x}+b)$; parameters minimize class-weighted cross-entropy.

\paragraph{Gradient Boosting (XGBoost).} We use XGBoost with imbalance-aware weighting~\citep{Chen2016_XGBoost}.

\paragraph{Multilayer Perceptron (MLP).} A feed-forward network with linear blocks, batch normalization, ReLU, and dropout~\citep{Ioffe2015_BN,Srivastava2014_Dropout}.

\paragraph{Transfer MLP (Multi-Head).}
We parameterize a shared backbone $f_\phi:\mathbb{R}^d\!\to\!\mathbb{R}^h$ and cancer-specific linear heads $\{h_c:\mathbb{R}^h\!\to\!\mathbb{R}\}_{c\in\mathcal{C}}$, yielding logits $z_c(x)=h_c(f_\phi(x))$. Head strategies for GC are detailed below.

\subsection{Multi-Cancer Pretraining Protocol}\label{sec:pretrain_multi}

Let $\mathcal{C}$ denote the set of \emph{non-gastric} cancers used for pretraining; in our main configuration, $\mathcal{C}=\{\mathrm{C},\mathrm{E},\mathrm{L},\mathrm{P}\}$. For $c\in\mathcal{C}$, let $\mathcal{D}_c=\{(x_i^{(c)},y_i^{(c)})\}_{i=1}^{N_c}$ be the corresponding case--control dataset built with identical features and a $1{:}3$ ratio. The final linear layer has $|\mathcal{C}|$ outputs during pretraining and is extended to $|\mathcal{C}|{+}1$ in the \texttt{new} strategy during fine-tuning (to add a GC head).

\paragraph{Objective and type-mixed minibatches.}
We minimize the average class-weighted binary cross-entropy over types:
\begin{equation}
\label{eq:pre_obj}
\min_{\phi,\{h_c\}}\ \mathcal{L}_{\mathrm{pre}}(\phi,\{h_c\})=\frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}} \ \mathbb{E}_{(x,y)\sim\mathcal{D}_c}\!\big[\,\ell_{\mathrm{BCE}}(h_c(f_\phi(x)),y;\alpha)\,\big],
\end{equation}
with positive-class weight $\alpha{=}3$. In practice, each optimization step draws one mini-batch $B_c$ from \emph{each} $c\in\mathcal{C}$ and accumulates losses across types:
\begin{equation}
\label{eq:pre_mix}
\hat{\mathcal{L}}_{\mathrm{pre}}=\frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}}\ \frac{1}{|B_c|}\sum_{(x,y)\in B_c}\ell_{\mathrm{BCE}}(h_c(f_\phi(x)),y;\alpha),
\end{equation}
so that heads specialize while gradients are shared through $f_\phi$. When training on a single type ($|\mathcal{C}|{=}1$), \eqref{eq:pre_mix} reduces to standard single-task training.

\paragraph{Validation across types and model selection.}
At regular intervals (every $K$ steps), we evaluate each head on its validation split and report both per-type metrics and their mean:
\begin{equation}
\overline{\mathrm{AUROC}}=\frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}}\mathrm{AUROC}_c,\qquad
\overline{\mathrm{AP}}=\frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}}\mathrm{AP}_c.
\end{equation}
We retain the snapshot that maximizes $\overline{\mathrm{AUROC}}$ (ties broken by $\overline{\mathrm{AP}}$). The per-type validation metrics $\{(\mathrm{AUROC}_c,\mathrm{AP}_c)\}$ are stored for GC head selection in the \texttt{best} strategy.

\paragraph{Fine-tuning head strategies.}
During GC fine-tuning we use one of three strategies to produce the scalar logit $s(x)$:
\begin{align}
\texttt{avg:}\quad & s(x)=\frac{1}{|\mathcal{C}|}\sum_{c\in\mathcal{C}} z_c(x),\\
\texttt{best:}\quad & s(x)=z_{\hat c}(x),\ \ \hat c=\arg\max_{c\in\mathcal{C}}\big(\mathrm{AUROC}_c,\mathrm{AP}_c\big),\\
\texttt{new:}\quad & s(x)=z_G(x),\ \ \text{extend the last layer to include a new GC head.}
\end{align}
In \texttt{new}, the GC head is randomly initialized; in all modes, the shared backbone is initialized from pretraining.

\paragraph{L2-SP anchoring and trainable sets.}
We regularize the distance to the pretrained parameters $\theta^\star$ using the L2-SP penalty~\citep{Li2018_L2SP}:
\begin{equation}
\Omega_{\mathrm{L2\text{-}SP}}(\theta)=\sum_{j\in\mathcal{S}}\|\theta_j-\theta^\star_j\|_2^2,
\end{equation}
where $\mathcal{S}$ excludes biases; in \texttt{new} we also exclude the expanded final linear layer so that the new GC head is unconstrained. The GC fine-tuning objective is
\begin{equation}
\label{eq:ft_obj}
\min_{\theta}\ \mathbb{E}_{(x,y)\sim \mathcal{D}_G}\!\big[\ell_{\mathrm{BCE}}(s(x),y;\alpha)\big]\;+\;\lambda\,\Omega_{\mathrm{L2\text{-}SP}}(\theta).
\end{equation}
When \texttt{ft\_head\_only} is enabled, a gradient mask restricts updates to the selected row of the final linear layer (the shared backbone runs in inference mode for weight updates).

\paragraph{Batch size, imbalance, and schedules.}
Mini-batch size is either fixed at $64$ or set to $\lfloor \texttt{mb\_rate}\cdot N\rfloor$ (capped to $[8,64]$). All tasks share $\alpha{=}3$ in $\ell_{\mathrm{BCE}}$. Adam optimizes both phases with separate learning rates/decays $(\texttt{pre\_lr},\texttt{pre\_weight\_decay})$ and $(\texttt{ft\_lr},\texttt{ft\_weight\_decay})$.

\paragraph{Summary.}
\begin{enumerate}[topsep=0pt,itemsep=1pt,leftmargin=12pt]
\item Build per-type loaders for each $c\in\mathcal{C}$; initialize $f_\phi$ and $\{h_c\}$.
\item For $t=1\ldots T$: draw one batch from each loader; compute \eqref{eq:pre_mix}; update parameters.
\item Every $K$ steps: compute per-type and mean validation metrics; select the best snapshot.
\item Save weights and per-type metrics; proceed to GC fine-tuning with \texttt{avg}/\texttt{best}/\texttt{new} and (optionally) L2-SP.
\end{enumerate}

\subsection{Losses, Class Imbalance, and Thresholding}
For a logit $u$ and label $y\in\{0,1\}$, we use
\begin{equation}
\ell_{\mathrm{BCE}}(u,y;\alpha)=-\alpha\,y\log\sigma(u)-(1-y)\log(1-\sigma(u)),
\end{equation}
with positive-class weight $\alpha{=}3$. Decision thresholds are chosen on the validation set by maximizing $F_1$:
\begin{equation}
t^\star=\arg\max_{t\in[0,1]}\ \frac{2\,\mathrm{Precision}(t)\cdot\mathrm{Recall}(t)}{\mathrm{Precision}(t)+\mathrm{Recall}(t)}.
\end{equation}

\subsection{Optimization and Early Stopping}
Neural models are optimized with Adam~\citep{Kingma2015_Adam}. Batch normalization and dropout act as regularizers~\citep{Ioffe2015_BN,Srivastava2014_Dropout}. In pretraining, validation is performed every $K$ steps and the best snapshot by $\overline{\mathrm{AUROC}}$ is retained; in fine-tuning, early stopping on validation AUROC is optional.

\subsection{Evaluation Metrics}
Primary metrics are AUROC and AP, with sensitivity and specificity reported at $t^\star$. In imbalanced settings, AP is often more informative than AUROC~\citep{Saito2015_PR}.

\subsection{Implementation and Reproducibility}
The pipeline is implemented in Python. The entry script enumerates a safe hyperparameter grid, caches pretrained configurations (to avoid redundant pretraining), and executes single-run jobs. Each run writes a JSON artifact with hyperparameters and metrics; merged summaries are produced for analysis. Deterministic seeding and graceful-stop flags are supported.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

Authors should discuss the results and how they can be interpreted from the perspective of previous studies and of the working hypotheses. The findings and their implications should be discussed in the broadest context possible. Future research directions may also be highlighted.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

This section is not mandatory, but can be added to the manuscript if the discussion is unusually long or complex.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Patents}

This section is not mandatory, but may be added if there are patents resulting from the work reported in this manuscript.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following supporting information can be downloaded at:  \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.}

% Only for journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title. A supporting video article is available at doi: link.}

% Only used for preprtints:
% \supplementary{The following supporting information can be downloaded at the website of this paper posted on \href{https://www.preprints.org/}{Preprints.org}.}

% Only for journal Hardware:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.\vspace{6pt}\\
%\begin{tabularx}{\textwidth}{lll}
%\toprule
%\textbf{Name} & \textbf{Type} & \textbf{Description} \\
%\midrule
%S1 & Python script (.py) & Script of python source code used in XX \\
%S2 & Text (.txt) & Script of modelling code used to make Figure X \\
%S3 & Text (.txt) & Raw data from experiment X \\
%S4 & Video (.mp4) & Video demonstrating the hardware in use \\
%... & ... & ... \\
%\bottomrule
%\end{tabularx}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{For research articles with several authors, a short paragraph specifying their individual contributions must be provided. The following statements should be used ``Conceptualization, X.X. and Y.Y.; methodology, X.X.; software, X.X.; validation, X.X., Y.Y. and Z.Z.; formal analysis, X.X.; investigation, X.X.; resources, X.X.; data curation, X.X.; writing---original draft preparation, X.X.; writing---review and editing, X.X.; visualization, X.X.; supervision, X.X.; project administration, X.X.; funding acquisition, Y.Y. All authors have read and agreed to the published version of the manuscript.'', please turn to the  \href{http://img.mdpi.org/data/contributor-role-instruction.pdf}{CRediT taxonomy} for the term explanation. Authorship must be limited to those who have contributed substantially to the work~reported.}

\funding{Please add: ``This research received no external funding'' or ``This research was funded by NAME OF FUNDER grant number XXX.'' and  and ``The APC was funded by XXX''. Check carefully that the details given are accurate and use the standard spelling of funding agency names at \url{https://search.crossref.org/funding}, any errors may affect your future funding.}

\institutionalreview{In this section, you should add the Institutional Review Board Statement and approval number, if relevant to your study. You might choose to exclude this statement if the study did not require ethical approval. Please note that the Editorial Office might ask you for further information. Please add “The study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving humans. OR “The animal study protocol was approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving animals. OR “Ethical review and approval were waived for this study due to REASON (please provide a detailed justification).” OR “Not applicable” for studies not involving humans or animals.}

\informedconsent{Any research article describing a study involving humans should contain this statement. Please add ``Informed consent was obtained from all subjects involved in the study.'' OR ``Patient consent was waived due to REASON (please provide a detailed justification).'' OR ``Not applicable'' for studies not involving humans. You might also choose to exclude this statement if the study did not involve humans.

Written informed consent for publication must be obtained from participating patients who can be identified (including by the patients themselves). Please state ``Written informed consent has been obtained from the patient(s) to publish this paper'' if applicable.}

\dataavailability{We encourage all authors of articles published in MDPI journals to share their research data. In this section, please provide details regarding where data supporting reported results can be found, including links to publicly archived datasets analyzed or generated during the study. Where no new data were created, or where data is unavailable due to privacy or ethical restrictions, a statement is still required. Suggested Data Availability Statements are available in section ``MDPI Research Data Policies'' at \url{https://www.mdpi.com/ethics}.} 

% Only for journal Drones
%\durcstatement{Current research is limited to the [please insert a specific academic field, e.g., XXX], which is beneficial [share benefits and/or primary use] and does not pose a threat to public health or national security. Authors acknowledge the dual-use potential of the research involving xxx and confirm that all necessary precautions have been taken to prevent potential misuse. As an ethical responsibility, authors strictly adhere to relevant national and international laws about DURC. Authors advocate for responsible deployment, ethical considerations, regulatory compliance, and transparent reporting to mitigate misuse risks and foster beneficial outcomes.}

% Only for journal Nursing Reports
%\publicinvolvement{Please describe how the public (patients, consumers, carers) were involved in the research. Consider reporting against the GRIPP2 (Guidance for Reporting Involvement of Patients and the Public) checklist. If the public were not involved in any aspect of the research add: ``No public involvement in any aspect of this research''.}
%
%% Only for journal Nursing Reports
%\guidelinesstandards{Please add a statement indicating which reporting guideline was used when drafting the report. For example, ``This manuscript was drafted against the XXX (the full name of reporting guidelines and citation) for XXX (type of research) research''. A complete list of reporting guidelines can be accessed via the equator network: \url{https://www.equator-network.org/}.}
%
%% Only for journal Nursing Reports
%\useofartificialintelligence{Please describe in detail any and all uses of artificial intelligence (AI) or AI-assisted tools used in the preparation of the manuscript. This may include, but is not limited to, language translation, language editing and grammar, or generating text. Alternatively, please state that “AI or AI-assisted tools were not used in drafting any aspect of this manuscript”.}

\acknowledgments{In this section you can acknowledge any support given which is not covered by the author contribution or funding sections. This may include administrative and technical support, or donations in kind (e.g., materials used for experiments). Where GenAI has been used for purposes such as generating text, data, or graphics, or for study design, data collection, analysis, or interpretation of data, please add “During the preparation of this manuscript/study, the author(s) used [tool name, version information] for the purposes of [description of use]. The authors have reviewed and edited the output and take full responsibility for the content of this publication.”}

\conflictsofinterest{Declare conflicts of interest or state ``The authors declare no conflicts of interest.'' Authors must identify and declare any personal circumstances or interest that may be perceived as inappropriately influencing the representation or interpretation of reported research results. Any role of the funders in the design of the study; in the collection, analyses or interpretation of data; in the writing of the manuscript; or in the decision to publish the results must be declared in this section. If there is no role, please state ``The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results''.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional

%% Only for journal Encyclopedia
%\entrylink{The Link to this entry published on the encyclopedia platform.}

\abbreviations{Abbreviations}{
The following abbreviations are used in this manuscript:
\\

\noindent 
\begin{tabular}{@{}ll}
MDPI & Multidisciplinary Digital Publishing Institute\\
DOAJ & Directory of open access journals\\
TLA & Three letter acronym\\
LD & Linear dichroism
\end{tabular}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
\appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixstart
\appendix
\section[\appendixname~\thesection]{}
\subsection[\appendixname~\thesubsection]{}
The appendix is an optional section that can contain details and data supplemental to the main text---for example, explanations of experimental details that would disrupt the flow of the main text but nonetheless remain crucial to understanding and reproducing the research shown; figures of replicates for experiments of which representative data are shown in the main text can be added here if brief, or as Supplementary Data. Mathematical proofs of results not central to the paper can be added as an appendix.

\begin{table}[H] 
\caption{This is a table caption.\label{tab5}}
%\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{CCC}
\toprule
\textbf{Title 1}	& \textbf{Title 2}	& \textbf{Title 3}\\
\midrule
Entry 1		& Data			& Data\\
Entry 2		& Data			& Data\\
\bottomrule
\end{tabularx}
\end{table}

\section[\appendixname~\thesection]{}
All appendix sections must be cited in the main text. In the appendices, Figures, Tables, etc. should be labeled, starting with ``A''---e.g., Figure A1, Figure A2, etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\isPreprints{}{% This command is only used for ``preprints''.
\begin{adjustwidth}{-\extralength}{0cm}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
\bibliography{references}

%=====================================
% References, variant B: internal bibliography
%=====================================


% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
%\isPreprints{}{% This command is only used for ``preprints''.
\end{adjustwidth}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
\end{document}

